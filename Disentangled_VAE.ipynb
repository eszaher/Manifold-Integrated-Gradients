{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29596b7b",
      "metadata": {
        "id": "29596b7b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
        "\n",
        "Tensor = TypeVar('torch.tensor')\n",
        "\n",
        "class BetaVAE(nn.Module):\n",
        "\n",
        "    num_iter = 0 # Global static variable to keep track of iterations\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels: int = 3,\n",
        "                 latent_dim: int = 4069,\n",
        "                 hidden_dims: List = None,\n",
        "                 beta: int = 4,\n",
        "                 gamma:float = 1000.,\n",
        "                 max_capacity: int = 25,\n",
        "                 Capacity_max_iter: int = 1e5,\n",
        "                 loss_type:str = 'H',\n",
        "                 **kwargs) -> None:\n",
        "        super(BetaVAE, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "        self.loss_type = loss_type\n",
        "        self.C_max = torch.Tensor([max_capacity])\n",
        "        self.C_stop_iter = Capacity_max_iter\n",
        "\n",
        "\n",
        "\n",
        "        modules = []\n",
        "        if hidden_dims is None:\n",
        "            hidden_dims = [32, 64, 128, 256, 512]\n",
        "            #hidden_dims = [16, 32, 64, 128, 256]\n",
        "\n",
        "        self.final_spatial_size = 64 // (2 ** len(hidden_dims))\n",
        "\n",
        "\n",
        "        # Build Encoder\n",
        "        for h_dim in hidden_dims:\n",
        "            modules.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
        "                              kernel_size= 3, stride= 2, padding  = 1),\n",
        "                    nn.BatchNorm2d(h_dim),\n",
        "                    nn.ELU())\n",
        "            )\n",
        "            in_channels = h_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(*modules)\n",
        "        #self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
        "        #self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
        "        self.fc_mu = nn.Linear(hidden_dims[-1]*self.final_spatial_size*self.final_spatial_size, latent_dim)\n",
        "        self.fc_var = nn.Linear(hidden_dims[-1]*self.final_spatial_size*self.final_spatial_size, latent_dim)\n",
        "\n",
        "\n",
        "\n",
        "        # Build Decoder\n",
        "        modules = []\n",
        "\n",
        "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * self.final_spatial_size * self.final_spatial_size)\n",
        "\n",
        "        hidden_dims.reverse()\n",
        "\n",
        "        for i in range(len(hidden_dims) - 1):\n",
        "            modules.append(\n",
        "                nn.Sequential(\n",
        "                    nn.ConvTranspose2d(hidden_dims[i],\n",
        "                                       hidden_dims[i + 1],\n",
        "                                       kernel_size=3,\n",
        "                                       stride = 2,\n",
        "                                       padding=1,\n",
        "                                       output_padding=1),\n",
        "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
        "                    nn.ELU())\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "        self.decoder = nn.Sequential(*modules)\n",
        "\n",
        "        self.final_layer = nn.Sequential(\n",
        "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
        "                                               hidden_dims[-1],\n",
        "                                               kernel_size=3,\n",
        "                                               stride=2,\n",
        "                                               padding=1,\n",
        "                                               output_padding=1),\n",
        "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
        "                            nn.ELU(),\n",
        "                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n",
        "                                      kernel_size= 3, padding= 1),\n",
        "                            nn.Tanh())\n",
        "\n",
        "    def encode(self, input: Tensor) -> List[Tensor]:\n",
        "        \"\"\"\n",
        "        Encodes the input by passing through the encoder network\n",
        "        and returns the latent codes.\n",
        "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
        "        :return: (Tensor) List of latent codes\n",
        "        \"\"\"\n",
        "        result = self.encoder(input)\n",
        "        #print(result.shape)\n",
        "        #spatial_size_after_encoder = result.shape[2]  # This will give you the spatial size after the encoder\n",
        "        #result = result.view(-1, 512, spatial_size_after_encoder, spatial_size_after_encoder)\n",
        "\n",
        "        result = torch.flatten(result, start_dim=1)\n",
        "\n",
        "        # Split the result into mu and var components\n",
        "        # of the latent Gaussian distribution\n",
        "        mu = self.fc_mu(result)\n",
        "        log_var = self.fc_var(result)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        #print(\"Z\", z.shape)\n",
        "\n",
        "        return [z, mu, log_var]\n",
        "\n",
        "    def decode(self, z: Tensor) -> Tensor:\n",
        "        result = self.decoder_input(z)\n",
        "        result = result.view(-1, 512, self.final_spatial_size, self.final_spatial_size)\n",
        "\n",
        "        #result = result.view(-1, 512, 2, 2)\n",
        "\n",
        "        result = self.decoder(result)\n",
        "        result = self.final_layer(result)\n",
        "        return result\n",
        "\n",
        "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Will a single z be enough ti compute the expectation\n",
        "        for the loss??\n",
        "        :param mu: (Tensor) Mean of the latent Gaussian\n",
        "        :param logvar: (Tensor) Standard deviation of the latent Gaussian\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps * std + mu\n",
        "\n",
        "    def forward(self, input: Tensor, **kwargs) -> Tensor:\n",
        "        z, mu, log_var = self.encode(input)\n",
        "\n",
        "\n",
        "        return  [self.decode(z), input, mu, log_var]\n",
        "\n",
        "    def loss_function(self,\n",
        "                      *args,\n",
        "                      **kwargs) -> dict:\n",
        "        self.num_iter += 1\n",
        "        recons = args[0]\n",
        "        input = args[1]\n",
        "        mu = args[2]\n",
        "        log_var = args[3]\n",
        "        feature_extractor = kwargs[\"extractor\"]\n",
        "\n",
        "        feat_in = torch.cat((recons, input), 0)\n",
        "        feature_loss = feature_extractor(feat_in)\n",
        "\n",
        "        kld_weight = kwargs['M_N']  # Account for the minibatch samples from the dataset\n",
        "\n",
        "        recons_loss =F.mse_loss(recons, input)\n",
        "\n",
        "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
        "\n",
        "        if self.loss_type == 'H': # https://openreview.net/forum?id=Sy2fzU9gl\n",
        "            loss = recons_loss + self.beta * kld_weight * kld_loss + feature_loss\n",
        "        elif self.loss_type == 'B': # https://arxiv.org/pdf/1804.03599.pdf\n",
        "            self.C_max = self.C_max.to(input.device)\n",
        "            C = torch.clamp(self.C_max/self.C_stop_iter * self.num_iter, 0, self.C_max.data[0])\n",
        "            loss = recons_loss + self.gamma * kld_weight* (kld_loss - C).abs() + feature_loss\n",
        "        else:\n",
        "            raise ValueError('Undefined loss type.')\n",
        "\n",
        "        return {'loss': loss, 'Reconstruction_Loss':recons_loss, 'KLD':kld_loss}\n",
        "\n",
        "    def sample(self,\n",
        "               num_samples:int,\n",
        "               current_device: int, **kwargs) -> Tensor:\n",
        "        \"\"\"\n",
        "        Samples from the latent space and return the corresponding\n",
        "        image space map.\n",
        "        :param num_samples: (Int) Number of samples\n",
        "        :param current_device: (Int) Device to run the model\n",
        "        :return: (Tensor)\n",
        "        \"\"\"\n",
        "        z = torch.randn(num_samples,\n",
        "                        self.latent_dim)\n",
        "\n",
        "        z = z.to(current_device)\n",
        "\n",
        "        samples = self.decode(z)\n",
        "        return samples\n",
        "\n",
        "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
        "        \"\"\"\n",
        "        Given an input image x, returns the reconstructed image\n",
        "        :param x: (Tensor) [B x C x H x W]\n",
        "        :return: (Tensor) [B x C x H x W]\n",
        "        \"\"\"\n",
        "\n",
        "        return self.forward(x)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eee9ba1",
      "metadata": {
        "id": "3eee9ba1"
      },
      "outputs": [],
      "source": [
        "\n",
        "class VGG19(nn.Module):\n",
        "    \"\"\"\n",
        "     Simplified version of the VGG19 \"feature\" block\n",
        "     This module's only job is to return the \"feature loss\" for the inputs\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channel_in=3, width=64):\n",
        "        super(VGG19, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(channel_in, width, 3, 1, 1)\n",
        "        self.conv2 = nn.Conv2d(width, width, 3, 1, 1)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(width, 2 * width, 3, 1, 1)\n",
        "        self.conv4 = nn.Conv2d(2 * width, 2 * width, 3, 1, 1)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(2 * width, 4 * width, 3, 1, 1)\n",
        "        self.conv6 = nn.Conv2d(4 * width, 4 * width, 3, 1, 1)\n",
        "        self.conv7 = nn.Conv2d(4 * width, 4 * width, 3, 1, 1)\n",
        "        self.conv8 = nn.Conv2d(4 * width, 4 * width, 3, 1, 1)\n",
        "\n",
        "        self.conv9 = nn.Conv2d(4 * width, 8 * width, 3, 1, 1)\n",
        "        self.conv10 = nn.Conv2d(8 * width, 8 * width, 3, 1, 1)\n",
        "        self.conv11 = nn.Conv2d(8 * width, 8 * width, 3, 1, 1)\n",
        "        self.conv12 = nn.Conv2d(8 * width, 8 * width, 3, 1, 1)\n",
        "\n",
        "        self.conv13 = nn.Conv2d(8 * width, 8 * width, 3, 1, 1)\n",
        "        self.conv14 = nn.Conv2d(8 * width, 8 * width, 3, 1, 1)\n",
        "        self.conv15 = nn.Conv2d(8 * width, 8 * width, 3, 1, 1)\n",
        "        self.conv16 = nn.Conv2d(8 * width, 8 * width, 3, 1, 1)\n",
        "\n",
        "        self.mp = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.load_params_()\n",
        "\n",
        "    def load_params_(self):\n",
        "        # Download and load Pytorch's pre-trained weights\n",
        "        state_dict = torch.hub.load_state_dict_from_url('https://download.pytorch.org/models/vgg19-dcbb9e9d.pth')\n",
        "        for ((name, source_param), target_param) in zip(state_dict.items(), self.parameters()):\n",
        "            target_param.data = source_param.data\n",
        "            target_param.requires_grad = False\n",
        "\n",
        "    def feature_loss(self, x):\n",
        "        return (x[:x.shape[0] // 2] - x[x.shape[0] // 2:]).pow(2).mean()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Expects x to be the target and source to concatenated on dimension 0\n",
        "        :return: Feature loss\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        loss = self.feature_loss(x)\n",
        "        x = self.conv2(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.mp(self.relu(x))  # 64x64\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv4(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.mp(self.relu(x))  # 32x32\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv6(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv7(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv8(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.mp(self.relu(x))  # 16x16\n",
        "\n",
        "        x = self.conv9(x)\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv10(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv11(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv12(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.mp(self.relu(x))  # 8x8\n",
        "\n",
        "        x = self.conv13(x)\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv14(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv15(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv16(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "\n",
        "        return loss/16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2890b7f3",
      "metadata": {
        "id": "2890b7f3"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.datasets as Datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.utils as vutils\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "\n",
        "dataset_root = \"\"\n",
        "save_dir = os.getcwd()\n",
        "model_name = \"STL10_betaVAE_Perceptual\"\n",
        "load_checkpoint  = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25c490a8",
      "metadata": {
        "id": "25c490a8"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "gpu_indx  = 0\n",
        "device = torch.device(gpu_indx if use_cuda else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5376b58",
      "metadata": {
        "id": "c5376b58"
      },
      "outputs": [],
      "source": [
        "def get_data_STL10(transform, batch_size, download = True, root = \"/data\"):\n",
        "    print(\"Loading trainset...\")\n",
        "    trainset = Datasets.STL10(root=root, split='unlabeled', transform=transform, download=download)\n",
        "\n",
        "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    print(\"Loading testset...\")\n",
        "    testset = Datasets.STL10(root=root, split='test', download=download, transform=transform)\n",
        "\n",
        "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    print(\"Done!\")\n",
        "\n",
        "    return trainloader, testloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6d434dd",
      "metadata": {
        "id": "d6d434dd",
        "outputId": "b7021404-587f-4cd3-91f9-c14265cc1646"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.Resize(image_size),\n",
        "                                transforms.CenterCrop(image_size),\n",
        "                                transforms.RandomHorizontalFlip(0.5),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(0.5, 0.5)])\n",
        "\n",
        "trainloader, testloader = get_data_STL10(transform, batch_size, download=True, root=dataset_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75b72f7d",
      "metadata": {
        "id": "75b72f7d",
        "outputId": "a1519b5f-58ff-4b0f-95a9-1ff0bd7cf99f"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(testloader)\n",
        "test_images, _ = next(dataiter)\n",
        "test_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7e6107e",
      "metadata": {
        "id": "b7e6107e",
        "outputId": "ff22d862-18ff-420d-802e-65f7be46c87f"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20,10))\n",
        "out = vutils.make_grid(test_images[0:8], normalize=True)\n",
        "plt.imshow(out.numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b785a280",
      "metadata": {
        "id": "b785a280",
        "outputId": "987379bc-bf13-472f-e769-37e33b0b7690"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "batch_size = 64\n",
        "image_size = 64\n",
        "lr = 0.005\n",
        "weight_decay= 0.0\n",
        "scheduler_gamma= 0.95\n",
        "kld_weight= 0.0025\n",
        "nepoch = 20\n",
        "num_epochs = 50\n",
        "\n",
        "from torchsummary import summary\n",
        "vae_net = BetaVAE().to(device)\n",
        "optimizer = optim.Adam(vae_net.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = scheduler_gamma)\n",
        "summary(vae_net, (3, 64, 64))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54596c84",
      "metadata": {
        "id": "54596c84",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "feature_extractor = VGG19().to(device)\n",
        "loss_log = []\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    vae_net.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(tqdm(trainloader, leave=False)):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        results = vae_net(data)\n",
        "        loss_dict = vae_net.loss_function(*results, M_N=kld_weight, extractor=feature_extractor)\n",
        "        loss = loss_dict['loss']\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "    avg_loss = train_loss / len(trainloader)  # Calculate average loss for the epoch\n",
        "    loss_log.append(avg_loss)  # Append average loss to loss_log\n",
        "    vae_net.eval()\n",
        "    with torch.no_grad():\n",
        "        recon_img, _, _,_ = vae_net(test_images.to(device))\n",
        "        img_cat = torch.cat((recon_img.cpu(), test_images), 2)\n",
        "\n",
        "        vutils.save_image(img_cat,\n",
        "                          \"%s/%s/%s_%d.png\" % (save_dir, \"Results\" , model_name, image_size),\n",
        "                          normalize=True)\n",
        "\n",
        "        #Save a checkpoint\n",
        "        torch.save({\n",
        "                    'epoch'                         : epoch,\n",
        "                    'loss_log'                      : loss_log,\n",
        "                    'model_state_dict'              : vae_net.state_dict(),\n",
        "                    'optimizer_state_dict'          : optimizer.state_dict()\n",
        "\n",
        "                     }, save_dir + \"/Models/\" + model_name + \"_\" + str(image_size) + \".pt\")\n",
        "\n",
        "    print(f'Epoch {epoch}/{num_epochs} - Avg Loss: {avg_loss}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "861d074e",
      "metadata": {
        "id": "861d074e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
