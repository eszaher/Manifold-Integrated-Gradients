{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "772fccb4",
      "metadata": {
        "id": "772fccb4"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.datasets as Datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.utils as vutils\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "batch_size = 64\n",
        "image_size = 64\n",
        "lr = 1e-4\n",
        "nepoch = 100\n",
        "start_epoch = 0\n",
        "dataset_root = \"\"\n",
        "save_dir = os.getcwd()\n",
        "model_name = \"STL10_Resnet_64\"\n",
        "latent_channels = 64\n",
        "load_checkpoint  = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09e16971",
      "metadata": {
        "id": "09e16971"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "\n",
        "\n",
        "class ResDown(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual down sampling block for the encoder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channel_in, channel_out, kernel_size=3):\n",
        "        super(ResDown, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channel_in, channel_out // 2, kernel_size, 2, kernel_size // 2)\n",
        "        self.bn1 = nn.BatchNorm2d(channel_out // 2, eps=1e-4)\n",
        "        self.conv2 = nn.Conv2d(channel_out // 2, channel_out, kernel_size, 1, kernel_size // 2)\n",
        "        self.bn2 = nn.BatchNorm2d(channel_out, eps=1e-4)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 2, kernel_size // 2)\n",
        "\n",
        "        self.act_fnc = nn.ELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip = self.conv3(x)\n",
        "        x = self.act_fnc(self.bn1(self.conv1(x)))\n",
        "        x = self.conv2(x)\n",
        "        return self.act_fnc(self.bn2(x + skip))\n",
        "\n",
        "\n",
        "class ResUp(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual up sampling block for the decoder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channel_in, channel_out, kernel_size=3, scale_factor=2):\n",
        "        super(ResUp, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(channel_in, channel_in // 2, kernel_size, 1, kernel_size // 2)\n",
        "        self.bn1 = nn.BatchNorm2d(channel_in // 2, eps=1e-4)\n",
        "        self.conv2 = nn.Conv2d(channel_in // 2, channel_out, kernel_size, 1, kernel_size // 2)\n",
        "        self.bn2 = nn.BatchNorm2d(channel_out, eps=1e-4)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 1, kernel_size // 2)\n",
        "\n",
        "        self.up_nn = nn.Upsample(scale_factor=scale_factor, mode=\"nearest\")\n",
        "\n",
        "        self.act_fnc = nn.ELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up_nn(x)\n",
        "        skip = self.conv3(x)\n",
        "        x = self.act_fnc(self.bn1(self.conv1(x)))\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        return self.act_fnc(self.bn2(x + skip))\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channel_in, channel_out, kernel_size=3):\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(channel_in, channel_in // 2, kernel_size, 1, kernel_size // 2)\n",
        "        self.bn1 = nn.BatchNorm2d(channel_in // 2, eps=1e-4)\n",
        "        self.conv2 = nn.Conv2d(channel_in // 2, channel_out, kernel_size, 1, kernel_size // 2)\n",
        "        self.bn2 = nn.BatchNorm2d(channel_out, eps=1e-4)\n",
        "\n",
        "        if not channel_in == channel_out:\n",
        "            self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 1, kernel_size // 2)\n",
        "        else:\n",
        "            self.conv3 = nn.Identity()\n",
        "\n",
        "        self.act_fnc = nn.ELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip = self.conv3(x)\n",
        "        x = self.act_fnc(self.bn1(self.conv1(x)))\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        return self.act_fnc(self.bn2(x + skip))\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, ch=64, blocks=(1, 2, 4, 8), latent_channels=512):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv_in = nn.Conv2d(channels, blocks[0] * ch, 3, 1, 1)\n",
        "\n",
        "        widths_in = list(blocks)\n",
        "        widths_out = list(blocks[1:]) + [blocks[-1]]\n",
        "\n",
        "        layer_blocks = []\n",
        "\n",
        "        for w_in, w_out in zip(widths_in, widths_out):\n",
        "            layer_blocks.append(ResDown(w_in * ch, w_out * ch))\n",
        "\n",
        "        layer_blocks.append(ResBlock(blocks[-1] * ch, blocks[-1] * ch))\n",
        "        layer_blocks.append(ResBlock(blocks[-1] * ch, blocks[-1] * ch))\n",
        "\n",
        "        self.res_blocks = nn.Sequential(*layer_blocks)\n",
        "\n",
        "        self.conv_mu = nn.Conv2d(blocks[-1] * ch, latent_channels, 1, 1)\n",
        "        self.conv_log_var = nn.Conv2d(blocks[-1] * ch, latent_channels, 1, 1)\n",
        "        self.act_fnc = nn.ELU()\n",
        "\n",
        "    def sample(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x, sample=False):\n",
        "        x = self.act_fnc(self.conv_in(x))\n",
        "        x = self.res_blocks(x)\n",
        "\n",
        "        mu = self.conv_mu(x)\n",
        "        log_var = self.conv_log_var(x)\n",
        "\n",
        "        if self.training or sample:\n",
        "            x = self.sample(mu, log_var)\n",
        "        else:\n",
        "            x = mu\n",
        "\n",
        "        return x, mu, log_var\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder block\n",
        "    Built to be a mirror of the encoder block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, ch=64, blocks=(1, 2, 4, 8), latent_channels=512):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv_in = nn.Conv2d(latent_channels, ch * blocks[-1], 1, 1)\n",
        "\n",
        "        widths_out = list(blocks)[::-1]\n",
        "        widths_in = (list(blocks[1:]) + [blocks[-1]])[::-1]\n",
        "\n",
        "        layer_blocks = [ResBlock(blocks[-1] * ch, blocks[-1] * ch),\n",
        "                        ResBlock(blocks[-1] * ch, blocks[-1] * ch)]\n",
        "\n",
        "        for w_in, w_out in zip(widths_in, widths_out):\n",
        "            layer_blocks.append(ResUp(w_in * ch, w_out * ch))\n",
        "\n",
        "        self.res_blocks = nn.Sequential(*layer_blocks)\n",
        "\n",
        "        self.conv_out = nn.Conv2d(blocks[0] * ch, channels, 3, 1, 1)\n",
        "        self.act_fnc = nn.ELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act_fnc(self.conv_in(x))\n",
        "        x = self.res_blocks(x)\n",
        "        mu = torch.tanh(self.conv_out(x))\n",
        "        return mu\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    \"\"\"\n",
        "    VAE network, uses the above encoder and decoder blocks\n",
        "    \"\"\"\n",
        "    def __init__(self, channel_in=3, ch=64, blocks=(1, 2, 4, 8), latent_channels=512):\n",
        "        super(VAE, self).__init__()\n",
        "        \"\"\"Res VAE Network\n",
        "        channel_in  = number of channels of the image\n",
        "        z = the number of channels of the latent representation\n",
        "        (for a 64x64 image this is the size of the latent vector)\n",
        "        \"\"\"\n",
        "\n",
        "        self.encoder = Encoder(channel_in, ch=ch, blocks=blocks, latent_channels=latent_channels)\n",
        "        self.decoder = Decoder(channel_in, ch=ch, blocks=blocks, latent_channels=latent_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoding, mu, log_var = self.encoder(x)\n",
        "        recon_img = self.decoder(encoding)\n",
        "        return recon_img, mu, log_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f70d4acc",
      "metadata": {
        "id": "f70d4acc",
        "outputId": "eb88f5d6-826f-459f-f2b4-517f7598d29f"
      },
      "outputs": [],
      "source": [
        "\n",
        "save_dir = os.getcwd()\n",
        "\n",
        "\n",
        "def get_data_STL10(transform, batch_size, download = True, root = \"/data\"):\n",
        "    print(\"Loading trainset...\")\n",
        "    trainset = Datasets.STL10(root=root, split='unlabeled', transform=transform, download=download)\n",
        "\n",
        "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    print(\"Loading testset...\")\n",
        "    testset = Datasets.STL10(root=root, split='test', download=download, transform=transform)\n",
        "\n",
        "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    print(\"Done!\")\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize(image_size),\n",
        "                                transforms.CenterCrop(image_size),\n",
        "                                transforms.RandomHorizontalFlip(0.5),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(0.5, 0.5)])\n",
        "\n",
        "trainloader, testloader = get_data_STL10(transform, batch_size, download=True, root=dataset_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a47c3cc2",
      "metadata": {
        "id": "a47c3cc2",
        "outputId": "6a9ab53f-b2e3-4ecc-961b-c1e5bd83026e"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "dataiter = iter(testloader)\n",
        "test_images, _ = next(dataiter)\n",
        "trainiter = iter(trainloader)\n",
        "train_images, _ = next(trainiter)\n",
        "\n",
        "plt.figure(figsize = (1,1))\n",
        "out = vutils.make_grid(train_images[42], normalize=True)\n",
        "plt.imshow(out.numpy().transpose((1, 2, 0)))\n",
        "\n",
        "test_images.shape\n",
        "\n",
        "num_samples = 30\n",
        "sample_indices = random.sample(range(test_images.shape[0]), num_samples)\n",
        "\n",
        "# Select those images\n",
        "sampled_images = test_images[sample_indices]\n",
        "\n",
        "# Average the sampled images\n",
        "\n",
        "average_image = torch.mean(sampled_images, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffcad259",
      "metadata": {
        "id": "ffcad259",
        "outputId": "19b78946-26f3-442a-b92a-fc778c0b98a9"
      },
      "outputs": [],
      "source": [
        "out = vutils.make_grid(average_image, normalize=True)\n",
        "\n",
        "plt.imshow(out.numpy().transpose((1, 2, 0)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94d40dc0",
      "metadata": {
        "id": "94d40dc0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7982ccc4",
      "metadata": {
        "id": "7982ccc4",
        "outputId": "4b1db0cc-73c4-453f-913c-2f628f057bf0"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "gpu_indx  = 0\n",
        "device = torch.device(gpu_indx if use_cuda else \"cpu\")\n",
        "\n",
        "vae_net = VAE(channel_in=3, ch=64, blocks=(1, 2, 4, 8), latent_channels=64).to(device)\n",
        "# setup optimizer\n",
        "optimizer = optim.Adam(vae_net.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "checkpoint = torch.load(save_dir + \"/Models/\" + model_name + \"_\" + str(image_size) + \".pt\")\n",
        "print(\"Checkpoint loaded\")\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "vae_net.load_state_dict(checkpoint['model_state_dict'])\n",
        "start_epoch = checkpoint[\"epoch\"]\n",
        "loss_log = checkpoint[\"loss_log\"]\n",
        "\n",
        "vae_net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2acd3637",
      "metadata": {
        "id": "2acd3637",
        "outputId": "18071058-1204-4fb7-94fc-adef95f84262",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "vae_net.eval()\n",
        "x,_,_ =vae_net.encoder(test_images[0].view(1,3,64,64).cuda())\n",
        "test_z,_,_ = vae_net.encoder(test_images[16].view(1,3,64,64).cuda())\n",
        "zero_z = torch.FloatTensor(1,64,4,4).zero_().cuda()\n",
        "rand_z = torch.randn_like(zero_z)\n",
        "x_black = torch.FloatTensor(1,3,64,64).zero_().cuda()\n",
        "x_white = torch.full((3,64,64), 1.0).view(1,3,64,64).cuda()\n",
        "print(x_white.shape, x_black.shape )\n",
        "z_black,_,_ = vae_net.encoder(x_black)\n",
        "z_white,_,_ = vae_net.encoder(x_white)\n",
        "average_z ,_,_ =vae_net.encoder(average_image.view(1,3,64,64).cuda())\n",
        "#igg = get_blurred_image(test_images[7].view(1,3,64,64).cuda(), sigma=20)\n",
        "#blurred_img = igg.view(1,3,64,64)\n",
        "#blurred_z = vae_net.encoder(blurred_img)\n",
        "\n",
        "def interpolate(start, end, steps):\n",
        "    \"\"\"Generate interpolated vectors between start and end.\"\"\"\n",
        "    interpolation = [start + (float(i)/steps)*(end-start) for i in range(0, steps)]\n",
        "    return interpolation\n",
        "\n",
        "# Interpolate between zero_z and x\n",
        "interpolated_vectors = interpolate(zero_z, x, 10)\n",
        "\n",
        "# Decode these vectors to images\n",
        "interpolated_rec_images = [vae_net.decoder(vec) for vec in interpolated_vectors]\n",
        "interpolated_images = interpolate(interpolated_rec_images[0], interpolated_rec_images[-1], 10)\n",
        "\n",
        "\n",
        "\n",
        "for inter_ing in interpolated_rec_images:\n",
        "    plt.figure(figsize = (1,1))\n",
        "\n",
        "    decoded = vutils.make_grid(inter_ing, normalize=True)\n",
        "    plt.imshow(decoded.cpu().numpy().transpose((1, 2, 0)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15cd9849",
      "metadata": {
        "id": "15cd9849",
        "outputId": "68e4f439-ec50-4609-f705-45b7f55b7401",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for inter_ing in interpolated_images:\n",
        "    plt.figure(figsize = (1,1))\n",
        "\n",
        "    decoded = vutils.make_grid(inter_ing, normalize=True)\n",
        "    plt.imshow(decoded.cpu().numpy().transpose((1, 2, 0)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0fd761f",
      "metadata": {
        "id": "e0fd761f",
        "outputId": "724e92ce-c370-45fa-9d4c-2fe1ad5b55be"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import gaussian_filter, center_of_mass\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "def get_blurred_image(image, sigma=10):\n",
        "    if len(image.shape) == 4:\n",
        "        image = image.cpu().numpy()\n",
        "        blurred_images = [gaussian_filter(im, (sigma, sigma, 0)) for im in image]\n",
        "        return torch.tensor(blurred_images).cuda()\n",
        "    elif len(image.shape) == 3:\n",
        "        return gaussian_filter(image, (sigma, sigma, 0))\n",
        "    else:\n",
        "        return gaussian_filter(image, sigma)\n",
        "\n",
        "plt.figure(figsize = (1,1))\n",
        "\n",
        "igg = get_blurred_image(test_images[7].view(1,3,64,64).cuda(), sigma=20)\n",
        "\n",
        "igg = vutils.make_grid(igg[0], normalize=True)\n",
        "#plt.imshow(igg.transpose((1, 2, 0)))\n",
        "\n",
        "plt.imshow(igg.cpu().numpy().transpose((1, 2, 0)))\n",
        "igg.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ts3FaMCEXrS3",
      "metadata": {
        "id": "Ts3FaMCEXrS3"
      },
      "outputs": [],
      "source": [
        "recon_img, _, _ = vae_net(igg.view(1,3,64,64))\n",
        "s = vae_net.decoder(z_black)\n",
        "#s = s.squeeze().cpu().detach().numpy().transpose((1, 2, 0))\n",
        "plt.figure(figsize = (1,1))\n",
        "\n",
        "\n",
        "decoded = vutils.make_grid(recon_img, normalize=True)\n",
        "plt.imshow(decoded.cpu().numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d55c773",
      "metadata": {
        "id": "9d55c773",
        "outputId": "d4f984b3-63c0-4d8f-9f6d-e72bba4769ca"
      },
      "outputs": [],
      "source": [
        "x_black.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e39d49dd",
      "metadata": {
        "id": "e39d49dd"
      },
      "outputs": [],
      "source": [
        "vae_net.eval()\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Step 2: Create a random tensor of shape (batch_size, latent_channels, height, width).\n",
        "# Let's take batch_size as 1 for simplicity and height and width as 1x1.\n",
        "batch_size = 1\n",
        "latent_channels = 64  # as defined in the VAE\n",
        "height, width = 4, 4  # typical for a latent space, unless your VAE encoder produces differently shaped latent space\n",
        "\n",
        "random_tensor = torch.randn(batch_size, latent_channels, height, width).cuda()\n",
        "\n",
        "# Step 3: Decode the random tensor using the VAE's decoder.\n",
        "with torch.no_grad():  # since we're not training now\n",
        "    decoded_image = vae_net.decoder(random_tensor)\n",
        "\n",
        "print(decoded_image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9119bb8c",
      "metadata": {
        "id": "9119bb8c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.manual_seed(0)\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def compute_etta(model, zi, zi_minus, zi_plus, dt):\n",
        "    #model.eval()\n",
        "    g_zi_minus = model.decoder(zi_minus).view(-1)\n",
        "    g_zi = model.decoder(zi).view(-1)\n",
        "    g_zi_plus = model.decoder(zi_plus).view(-1)\n",
        "\n",
        "    finite_diff = (g_zi_plus - 2*g_zi + g_zi_minus) / dt\n",
        "\n",
        "\n",
        "\n",
        "    # Use the encoder's Jacobian to map the finite difference in X space back to Z space\n",
        "    Jh_tuple = torch.autograd.functional.jacobian(model.encoder, g_zi.view(1,3,image_size,image_size))\n",
        "    Jh = Jh_tuple[0].view(512*4*4, -1)  # Reshape to a 2D tensor\n",
        "\n",
        "    etta_i = - torch.mm(Jh, finite_diff.unsqueeze(-1)).view_as(zi)\n",
        "    del Jh\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return etta_i\n",
        "\n",
        "def compute_etta_d(model, zi, zi_minus, zi_plus, dt):\n",
        "    # Compute the finite difference\n",
        "    g_zi_minus = model.decoder(zi_minus).view(-1)\n",
        "    g_zi = model.decoder(zi).view(-1)\n",
        "    g_zi_plus = model.decoder(zi_plus).view(-1)\n",
        "\n",
        "    finite_diff = (g_zi_plus - 2 * g_zi + g_zi_minus) / dt\n",
        "    finite_diff = finite_diff.view(1, 3, image_size, image_size)  # Reshape it to match the encoder's output shape\n",
        "\n",
        "    scaled_finite_diff = 0.1 * finite_diff\n",
        "    #print(\"Fdidd shape\", finite_diff.shape)\n",
        "\n",
        "    # Compute Jacobian-vector product\n",
        "    vjp_outputs = torch.autograd.functional.vjp(model.decoder, zi, finite_diff)\n",
        "    #print(\"vjp\", len(vjp_outputs))\n",
        "    # Get the result from the vjp outputs\n",
        "    Jv = vjp_outputs[1].view_as(zi)\n",
        "    #print(\"Jv\", Jv.shape)\n",
        "\n",
        "    # Compute etta_i\n",
        "    etta_i = -Jv #+ scaled_finite_diff)\n",
        "\n",
        "    # Free up memory\n",
        "    del g_zi_minus, g_zi, g_zi_plus, finite_diff, Jv, vjp_outputs\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return etta_i\n",
        "\n",
        "def compute_etta1(model, zi, zi_minus, zi_plus, dt):\n",
        "\n",
        "    g_zi_minus = model.decoder(zi_minus).view(-1).detach()\n",
        "    g_zi = model.decoder(zi).view(-1)  # We will need the gradient information for this tensor for the Jacobian\n",
        "    g_zi_plus = model.decoder(zi_plus).view(-1).detach()\n",
        "\n",
        "    finite_diff = (g_zi_plus - 2 * g_zi + g_zi_minus) / dt\n",
        "\n",
        "    def partial_encoder(input_data):\n",
        "        return model.encoder(input_data)[1]\n",
        "\n",
        "    #encoder_output = model.encoder(model.decoder(zi))  # Assuming input_data is your input to the encoder\n",
        "      # Access the first element of the tuple\n",
        "    Jh_tuple = torch.autograd.functional.jacobian(partial_encoder, g_zi.view(1, 3, 64, 64))\n",
        "    Jh = Jh_tuple.view(512 * 4 * 4, -1)  # Reshape to a 2D tensor\n",
        "\n",
        "\n",
        "    # Use the encoder's Jacobian to map the finite difference in X space back to Z space\n",
        "    #Jh_tuple = torch.autograd.functional.jacobian(model.encoder, g_zi.view(1, 3, 64, 64))\n",
        "    #Jh = Jh_tuple[0].view(512 * 4 * 4, -1)  # Reshape to a 2D tensor\n",
        "\n",
        "    etta_i = - torch.mm(Jh, finite_diff.unsqueeze(-1)).view_as(zi)\n",
        "\n",
        "    # Free up memory\n",
        "    del g_zi_minus, g_zi, g_zi_plus, Jh, Jh_tuple, finite_diff\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return etta_i\n",
        "\n",
        "\n",
        "def compute_etta(model, zi, zi_minus, zi_plus, dt):\n",
        "    # Compute the finite difference\n",
        "    g_zi_minus = model.decoder(zi_minus).view(-1)\n",
        "    g_zi = model.decoder(zi).view(-1)\n",
        "    g_zi_plus = model.decoder(zi_plus).view(-1)\n",
        "\n",
        "    finite_diff = (g_zi_plus - 2 * g_zi + g_zi_minus) / dt\n",
        "    finite_diff = finite_diff.view(1, 3, image_size, image_size)  # Reshape it to match the encoder's output shape\n",
        "\n",
        "    # Define a wrapper function for the encoder, so it can handle just the required output\n",
        "    def partial_encoder(input_data):\n",
        "        return model.encoder(input_data)[1]\n",
        "\n",
        "    # Compute Jacobian-vector product\n",
        "    #vjp_outputs = torch.autograd.functional.jvp(wrapper_decoder, zi, finite_diff)\n",
        "    vjp_outputs = torch.autograd.functional.jvp(partial_encoder, g_zi.view(1, 3, image_size, image_size), finite_diff)\n",
        "    # vjp_outputs = torch.autograd.functional.jvp(wrapper_func, g_zi.view(1, 3, 64, 64), finite_diff)\n",
        "\n",
        "    # Get the result from the vjp outputs\n",
        "    Jv = vjp_outputs[1].view_as(zi)\n",
        "\n",
        "    # Compute etta_i\n",
        "    etta_i = -Jv\n",
        "\n",
        "    # Free up memory\n",
        "    del g_zi_minus, g_zi, g_zi_plus, finite_diff, Jv, vjp_outputs\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return etta_i\n",
        "\n",
        "\n",
        "def compute_etta_d(model, zi, zi_minus, zi_plus, dt):\n",
        "    # Compute the finite difference\n",
        "    g_zi_minus = model.decoder(zi_minus).view(-1)\n",
        "    g_zi = model.decoder(zi).view(-1)\n",
        "    g_zi_plus = model.decoder(zi_plus).view(-1)\n",
        "\n",
        "    finite_diff = (g_zi_plus - 2 * g_zi + g_zi_minus) / dt\n",
        "    finite_diff = finite_diff.view(1, 3, image_size, image_size)  # Reshape it to match the encoder's output shape\n",
        "\n",
        "    # Compute Jacobian-vector product\n",
        "    vjp_outputs = torch.autograd.functional.vjp(model.decoder, zi, finite_diff)\n",
        "    #print(\"vjp\", len(vjp_outputs))\n",
        "    # Get the result from the vjp outputs\n",
        "    Jv = vjp_outputs[1].view_as(zi)\n",
        "\n",
        "    # Compute etta_i\n",
        "    etta_i = -Jv\n",
        "\n",
        "    # Free up memory\n",
        "    del g_zi_minus, g_zi, g_zi_plus, finite_diff, Jv, vjp_outputs\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return etta_i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef580de9",
      "metadata": {
        "id": "ef580de9",
        "outputId": "d15eba43-734b-4db5-b240-c0e5516110b8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def backtracking_line_search(model, z_collection, i, direction, start_alpha, beta, dt, max_iterations, c=0.001):\n",
        "    alpha = start_alpha\n",
        "    current_energy = sum_of_etta_norms(model, z_collection, dt)\n",
        "    gradient_norm_square = direction.norm().pow(2)\n",
        "\n",
        "\n",
        "    tmp_z = z_collection[i] - alpha * direction\n",
        "    new_z_collection = [element.clone() for element in z_collection]\n",
        "    new_z_collection[i] = tmp_z\n",
        "    #iterations_count = 0\n",
        "\n",
        "    while sum_of_etta_norms(model, new_z_collection, dt) > current_energy - c * alpha * gradient_norm_square:\n",
        "        alpha *= beta\n",
        "        tmp_z = z_collection[i] - alpha * direction\n",
        "        new_z_collection[i] = tmp_z\n",
        "\n",
        "    return alpha\n",
        "\n",
        "def sum_of_etta_norms_enc(model, z_collection, dt):\n",
        "    norms = []\n",
        "    for j in range(1, len(z_collection) - 1):\n",
        "        etta_j = compute_etta(model, z_collection[j], z_collection[j-1], z_collection[j+1], dt)\n",
        "        norms.append(etta_j.norm().pow(2).item())\n",
        "        del etta_j\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    return sum(norms)\n",
        "\n",
        "def sum_of_etta_norms(model, z_collection, dt):\n",
        "    norms = []\n",
        "    for j in range(1, len(z_collection) - 1):\n",
        "        etta_j = compute_etta_d(model, z_collection[j], z_collection[j-1], z_collection[j+1], dt)\n",
        "        norms.append(etta_j.norm().pow(2).item())\n",
        "        #del etta_j\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    return sum(norms)\n",
        "\n",
        "\n",
        "def geodesic_path_algorithm(model, z0, zT, alpha, T, beta, epsilon, max_iterations):\n",
        "    dt = 1.0 / T\n",
        "    z_collection = [z0 + float(i) / T * (zT - z0) for i in range(T)]\n",
        "    initial_sum_norms = float('inf')\n",
        "    iterations = 0\n",
        "\n",
        "    while sum_of_etta_norms(model, z_collection, dt) > epsilon:\n",
        "        print(f\"It{iterations}:Energy\", sum_of_etta_norms(model, z_collection, dt))\n",
        "\n",
        "    #while True:\n",
        "        etta_norms = []\n",
        "        if sum_of_etta_norms(model, z_collection, dt) > initial_sum_norms:\n",
        "                initial_sum_norms = sum_of_etta_norms(model, z_collection, dt)\n",
        "        else:\n",
        "            pass\n",
        "            #break\n",
        "        #while True:\n",
        "            #if init == 0:\n",
        "        if iterations == max_iterations:\n",
        "            break\n",
        "\n",
        "        iterations +=1\n",
        "\n",
        "        for i in range(1, T-1):\n",
        "            etta_i = compute_etta_d(model, z_collection[i], z_collection[i-1], z_collection[i+1], dt)\n",
        "            #alpha_i = backtracking_line_search(model, z_collection, i, etta_i, alpha, beta, dt, max_iterations)\n",
        "            alpha_i= alpha\n",
        "            z_collection[i] -= alpha_i * etta_i\n",
        "            etta_norms.append(etta_i.norm().pow(2).item())\n",
        "\n",
        "            del etta_i\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "    return z_collection\n",
        "\n",
        "# Example usage:\n",
        "x,_,_ =vae_net.encoder(train_images[1].view(1,3,64,64).cuda())\n",
        "zero_z = torch.FloatTensor(1,64,4,4).zero_().cuda()\n",
        "z_0,_,_ = vae_net.encoder(train_images[3].view(1,3,64,64).cuda())\n",
        "\n",
        "path = geodesic_path_algorithm(vae_net, zero_z, x, alpha=0.00001, T=10, beta=0.5, epsilon=1000, max_iterations=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb4d4ebb",
      "metadata": {
        "id": "eb4d4ebb",
        "outputId": "8ed3a2af-91e0-4eec-cfb9-d7e4d0ce80a3"
      },
      "outputs": [],
      "source": [
        "zero_z.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7d05450",
      "metadata": {
        "id": "f7d05450",
        "outputId": "462d4de2-1351-4a40-b698-bed75b392536",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "interpolated_geodesic_images = [vae_net.decoder(vec) for vec in path]\n",
        "\n",
        "vv = [v.squeeze() for v in interpolated_geodesic_images]\n",
        "plt.figure(figsize = (20,10))\n",
        "out = vutils.make_grid(vv, normalize=True)\n",
        "plt.imshow(out.cpu().numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2416ff51",
      "metadata": {
        "id": "2416ff51",
        "outputId": "b3040e9e-8fc5-4438-e592-28c4f07e75fc"
      },
      "outputs": [],
      "source": [
        "interpolated_vectors = interpolate(z_0, x, 10)\n",
        "\n",
        "# Decode these vectors to images\n",
        "interpolated_rec_images = [vae_net.decoder(vec) for vec in interpolated_vectors]\n",
        "\n",
        "vv = [v.squeeze() for v in interpolated_rec_images]\n",
        "plt.figure(figsize = (20,10))\n",
        "out = vutils.make_grid(vv, normalize=True)\n",
        "plt.imshow(out.cpu().numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d9f55a9",
      "metadata": {
        "id": "1d9f55a9",
        "outputId": "892d0ab9-ab5f-426d-b53f-020268087d24"
      },
      "outputs": [],
      "source": [
        "#x,_,_ =vae_net.encoder(train_images[1].view(1,3,64,64).cuda())\n",
        "#zero_z = torch.FloatTensor(1,64,4,4).zero_().cuda()\n",
        "#z_0,_,_ = vae_net.encoder(train_images[32].view(1,3,64,64).cuda())\n",
        "interpolated_vectors = interpolate(z_0, x, 10)\n",
        "\n",
        "\n",
        "\n",
        "vv = [v.squeeze() for v in interpolated_rec_images]\n",
        "plt.figure(figsize = (20,10))\n",
        "out = vutils.make_grid(vv, normalize=False)\n",
        "plt.imshow(out.cpu().numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68551de2",
      "metadata": {
        "id": "68551de2",
        "outputId": "8bbc4c97-2fd7-4f13-e10c-1b17eec3ade7",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "interpolated_images = interpolate(interpolated_rec_images[0], interpolated_rec_images[-1], 30)\n",
        "\n",
        "vv = [v.squeeze() for v in interpolated_images]\n",
        "plt.figure(figsize = (20,10))\n",
        "out = vutils.make_grid(vv, normalize=True)\n",
        "plt.imshow(out.cpu().numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a0667e6",
      "metadata": {
        "id": "7a0667e6",
        "outputId": "158bea21-9d8a-47fa-bf86-bc33fb8757af",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "x,_,_ =vae_net.encoder(train_images[42].view(1,3,64,64).cuda())\n",
        "zero_z = torch.FloatTensor(1,64,4,4).zero_().cuda()\n",
        "rand_z = torch.randn_like(zero_z)\n",
        "x_black = torch.FloatTensor(1,3,64,64).zero_().cuda()\n",
        "z_black,_,_ = vae_net.encoder(x_black)\n",
        "\n",
        "def interpolate(start, end, steps):\n",
        "    \"\"\"Generate interpolated vectors between start and end.\"\"\"\n",
        "    interpolation = [start + (float(i)/steps)*(end-start) for i in range(0, steps)]\n",
        "    return interpolation\n",
        "\n",
        "# Interpolate between zero_z and x\n",
        "interpolated_vectors = interpolate(zero_z, x, 10)\n",
        "\n",
        "# Decode these vectors to images\n",
        "interpolated_rec_images = [vae_net.decoder(vec) for vec in interpolated_vectors]\n",
        "interpolated_images = interpolate(interpolated_rec_images[0], interpolated_rec_images[-1], 10)\n",
        "\n",
        "for inter_ing in interpolated_rec_images:\n",
        "    plt.figure(figsize = (1,1))\n",
        "\n",
        "    decoded = vutils.make_grid(inter_ing, normalize=True)\n",
        "    plt.imshow(decoded.cpu().numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a04129fe",
      "metadata": {
        "id": "a04129fe",
        "outputId": "77928cc5-1427-4fa9-8068-6f4a8ab144af"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20,10))\n",
        "out = vutils.make_grid(test_images[0:8], normalize=True)\n",
        "plt.imshow(out.numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96ba3b7c",
      "metadata": {
        "id": "96ba3b7c",
        "outputId": "00f44bab-eaa8-4486-bcc6-6d9c20494c9e"
      },
      "outputs": [],
      "source": [
        "vv = [v.squeeze() for v in interpolated_geodesic_images]\n",
        "plt.figure(figsize = (20,10))\n",
        "out = vutils.make_grid(vv, normalize=True)\n",
        "plt.imshow(out.cpu().numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a6e1a94",
      "metadata": {
        "id": "4a6e1a94",
        "outputId": "1c3a4b82-8e4e-497b-a5af-a01ecd42d351",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "interpolated_vectors = interpolate(test_z, x, 10)\n",
        "interpolated_rec_images = [vae_net.decoder(vec) for vec in interpolated_vectors]\n",
        "interpolated_images = interpolate(interpolated_rec_images[0], interpolated_rec_images[-1], 10)\n",
        "\n",
        "plt.figure(figsize = (20,20))\n",
        "\n",
        "in_rec = [v.squeeze() for v in interpolated_rec_images]\n",
        "decoded = vutils.make_grid(in_rec, normalize=True)\n",
        "plt.imshow(decoded.cpu().numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43c6f846",
      "metadata": {
        "id": "43c6f846",
        "outputId": "cf3fbd19-f087-491d-9bc2-3d01b890f11c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "interpolated_vectors = interpolate(test_z, x, 10)\n",
        "interpolated_rec_images = [vae_net.decoder(vec) for vec in interpolated_vectors]\n",
        "interpolated_images = interpolate(interpolated_rec_images[0], interpolated_rec_images[-1], 10)\n",
        "\n",
        "plt.figure(figsize = (20,5))\n",
        "\n",
        "in_rec = [v.squeeze() for v in interpolated_images]\n",
        "decoded = vutils.make_grid(in_rec, normalize=True)\n",
        "plt.imshow(decoded.cpu().numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e5a6c90",
      "metadata": {
        "id": "9e5a6c90",
        "outputId": "c8de896a-a7e2-4e7f-ad9e-63f0c6060a3e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# A trial to fast compute geodesics, FAILED! left to have a look later\n",
        "\n",
        "import torch.cuda as cuda\n",
        "\n",
        "\n",
        "device = 'cuda:0'\n",
        "vae_net = vae_net.to(device)\n",
        "vae_net = nn.DataParallel(vae_net)\n",
        "\n",
        "import multiprocessing\n",
        "\n",
        "try:\n",
        "    multiprocessing.set_start_method('spawn')\n",
        "except RuntimeError:\n",
        "    pass\n",
        "\n",
        "\n",
        "def process_compute_etta(queue, device, model, zi, zi_minus, zi_plus, dt):\n",
        "    torch.cuda.set_device(device)\n",
        "    model.eval()\n",
        "\n",
        "    zi, zi_minus, zi_plus = zi.to(device), zi_minus.to(device), zi_plus.to(device)\n",
        "\n",
        "    etta = compute_etta(model, zi, zi_minus, zi_plus, dt)\n",
        "    queue.put(etta.cpu().numpy())\n",
        "\n",
        "\n",
        "def async_compute_etta(device, model, zi, zi_minus, zi_plus, dt):\n",
        "    with torch.cuda.device(device):  # Set the current device to this GPU\n",
        "        g_zi_minus = model.decoder(zi_minus).view(-1)\n",
        "        g_zi = model.decoder(zi).view(-1)\n",
        "        g_zi_plus = model.decoder(zi_plus).view(-1)\n",
        "        finite_diff = (g_zi_plus - 2 * g_zi + g_zi_minus) / dt\n",
        "        Jh_tuple = torch.autograd.functional.jacobian(model.encoder, g_zi.view(1, 3, 64, 64))\n",
        "        Jh = Jh_tuple[0].view(512 * 4 * 4, -1)\n",
        "        etta_i = - torch.mm(Jh, finite_diff.unsqueeze(-1)).view_as(zi)\n",
        "    return etta_i\n",
        "\n",
        "from multiprocessing import Queue, Process\n",
        "\n",
        "def geodesic_path_algorithm(model, z0, zT, alpha, T, epsilon=200):\n",
        "    dt = 1.0 / T\n",
        "    z_collection = [z0 + i / T * (zT - z0) for i in range(T+1)]\n",
        "\n",
        "    while True:\n",
        "        etta_norms = []\n",
        "\n",
        "        for i in range(1, T):\n",
        "            device = 'cuda:0' if i % 2 == 0 else 'cuda:1'\n",
        "\n",
        "            # Using multiprocessing to parallelize across GPUs\n",
        "            q = Queue()\n",
        "            p = Process(target=process_compute_etta, args=(q, device, model, z_collection[i], z_collection[i-1], z_collection[i+1], dt))\n",
        "            p.start()\n",
        "            etta_i = torch.tensor(q.get()).to(device)\n",
        "            p.join()\n",
        "\n",
        "            etta_norms.append(etta_i.norm().item())\n",
        "            z_collection[i] -= alpha * etta_i\n",
        "            del etta_i\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        if sum(etta_norms) < epsilon:\n",
        "            break\n",
        "\n",
        "        print(\"Energy\", sum(etta_norms))\n",
        "\n",
        "    return z_collection\n",
        "\n",
        "\n",
        "\n",
        "path = geodesic_path_algorithm(vae_net, z_black, x, alpha=0.01, T=10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7462127b",
      "metadata": {
        "id": "7462127b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
