{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nll(self, data, n_samples=1, batch_size=100):\n",
    "        \"\"\"\n",
    "        Function computed the estimate negative log-likelihood of the model. It uses importance\n",
    "        sampling method with the approximate posterior distribution. This may take a while.\n",
    "\n",
    "        Args:\n",
    "            data (torch.Tensor): The input data from which the log-likelihood should be estimated.\n",
    "                Data must be of shape [Batch x n_channels x ...]\n",
    "            n_samples (int): The number of importance samples to use for estimation\n",
    "            batch_size (int): The batchsize to use to avoid memory issues\n",
    "        \"\"\"\n",
    "\n",
    "        if n_samples <= batch_size:\n",
    "            n_full_batch = 1\n",
    "        else:\n",
    "            n_full_batch = n_samples // batch_size\n",
    "            n_samples = batch_size\n",
    "\n",
    "        log_p = []\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            x = data[i].unsqueeze(0)\n",
    "            encoder_output = self.encoder(x)\n",
    "            mu, log_var = encoder_output.embedding, encoder_output.log_covariance\n",
    "\n",
    "            log_p_x = []\n",
    "\n",
    "            for j in range(n_full_batch):\n",
    "                x_rep = torch.cat(batch_size * [x])\n",
    "\n",
    "                encoder_output = self.encoder(x_rep)\n",
    "                mu, log_var = encoder_output.embedding, encoder_output.log_covariance\n",
    "\n",
    "                std = torch.exp(0.5 * log_var)\n",
    "                z, eps = self._sample_gauss(mu, std)\n",
    "\n",
    "                log_q_z_given_x = -0.5 * (\n",
    "                    log_var + (z - mu) ** 2 / torch.exp(log_var)\n",
    "                ).sum(dim=-1)\n",
    "                log_p_z = self._log_p_z(z)\n",
    "\n",
    "                recon_x = self.decoder(z)[\"reconstruction\"]\n",
    "\n",
    "                #if self.model_config.reconstruction_loss == \"mse\":\n",
    "                log_p_x_given_z = -0.5 * F.mse_loss(\n",
    "                    recon_x.reshape(x_rep.shape[0], -1),\n",
    "                    x_rep.reshape(x_rep.shape[0], -1),\n",
    "                    reduction=\"none\",\n",
    "                ).sum(dim=-1) - torch.tensor(\n",
    "                    [np.prod(self.input_dim) / 2 * np.log(np.pi * 2)]\n",
    "                ).to(\n",
    "                    data.device)  # decoding distribution is assumed unit variance  N(mu, I)\n",
    "\n",
    "                #elif self.model_config.reconstruction_loss == \"bce\":\n",
    "                #    log_p_x_given_z = -F.binary_cross_entropy(\n",
    "                #        recon_x.reshape(x_rep.shape[0], -1),\n",
    "                #        x_rep.reshape(x_rep.shape[0], -1),\n",
    "                #        reduction=\"none\",\n",
    "                #    ).sum(dim=-1)\n",
    "\n",
    "                log_p_x.append(\n",
    "                    log_p_x_given_z + log_p_z - log_q_z_given_x\n",
    "                )  # log(2*pi) simplifies\n",
    "\n",
    "            log_p_x = torch.cat(log_p_x)\n",
    "\n",
    "            log_p.append((torch.logsumexp(log_p_x, 0) - np.log(len(log_p_x))).item())\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Current nll at {i}: {np.mean(log_p)}\")\n",
    "        return np.mean(log_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb6294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as Datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torchvision.transforms import functional as TF\n",
    "import PIL\n",
    "\n",
    "latent_channels = 64\n",
    "batch_size = 64\n",
    "image_size = 128\n",
    "lr = 5e-4\n",
    "nepoch = 100\n",
    "start_epoch = 0\n",
    "dataset_root = \"\"\n",
    "model_name = \"oxford_pets_VAE_VAMP\" \n",
    "torch.manual_seed(7)  # For reproducibility\n",
    "\n",
    "save_dir = os.getcwd()\n",
    "load_checkpoint  = True\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "gpu_indx  = 0\n",
    "device = torch.device(gpu_indx if use_cuda else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb5b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data_dir, file_names, transform=None, num_black_images=350, num_white_images=350, image_size=(3, 128, 128) ):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.file_names = file_names\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # Adad placeholders for black images\n",
    "        self.black_image_placeholder = \"<black_image>\"\n",
    "        self.file_names.extend([self.black_image_placeholder] * num_black_images)\n",
    "        self.white_image_placeholder = \"<white_image>\"\n",
    "        self.file_names.extend([self.white_image_placeholder] * num_white_images)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.file_names[idx] == self.black_image_placeholder:\n",
    "            # Create a black image\n",
    "            #black_image = torch.zeros(3, 256, 256)  # Adjust size as needed\n",
    "            black_image = torch.zeros(self.image_size)\n",
    "            black_image = TF.to_pil_image(black_image)\n",
    "            return transforms.ToTensor()(black_image) #self.transform(black_image) if self.transform else black_image\n",
    "        \n",
    "        if self.file_names[idx] == self.white_image_placeholder:\n",
    "            # Create a black image\n",
    "            #black_image = torch.zeros(3, 256, 256)  # Adjust size as needed\n",
    "            white_image = torch.ones(self.image_size)\n",
    "            white_image = TF.to_pil_image(white_image)\n",
    "            return transforms.ToTensor()(white_image) #self.transform(white_image) if self.transform else black_image\n",
    "\n",
    "        \n",
    "        img_name = os.path.join(self.data_dir, self.file_names[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(128, 128)),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize(size=(128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8362b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming all your images are in 'data_dir'\n",
    "all_images = [img for img in os.listdir(dataset_root) if img.endswith('.jpg')]  # Adjust for your file type\n",
    "train_images, test_images = train_test_split(all_images, test_size=0.05, random_state=7)\n",
    "\n",
    "train_dataset = CustomImageDataset(data_dir=dataset_root, file_names=train_images, transform=train_transform, num_black_images=500)\n",
    "test_dataset = CustomImageDataset(data_dir=dataset_root, file_names=test_images, transform=test_transform, num_black_images=500)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ef0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf563dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "train_images = next(dataiter)\n",
    "train_images.shape\n",
    "\n",
    "test_dataiter = iter(test_loader)\n",
    "test_images = next(test_dataiter)\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b99a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(test_images[0:64], normalize=True)\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e30f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "class ResDown(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual down sampling block for the encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channel_in, channel_out, kernel_size=3):\n",
    "        super(ResDown, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channel_in, channel_out // 2, kernel_size, 2, kernel_size // 2)\n",
    "        self.bn1 = nn.BatchNorm2d(channel_out // 2, eps=1e-4)\n",
    "        self.conv2 = nn.Conv2d(channel_out // 2, channel_out, kernel_size, 1, kernel_size // 2)\n",
    "        self.bn2 = nn.BatchNorm2d(channel_out, eps=1e-4)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 2, kernel_size // 2)\n",
    "\n",
    "        self.act_fnc = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip = self.conv3(x)\n",
    "        x = self.act_fnc(self.bn1(self.conv1(x)))\n",
    "        x = self.conv2(x)\n",
    "        return self.act_fnc(self.bn2(x+ skip))\n",
    "\n",
    "\n",
    "class ResUp(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual up sampling block for the decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channel_in, channel_out, kernel_size=3, scale_factor=2):\n",
    "        super(ResUp, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channel_in, channel_in // 2, kernel_size, 1, kernel_size // 2)\n",
    "        self.bn1 = nn.BatchNorm2d(channel_in // 2, eps=1e-4)\n",
    "        self.conv2 = nn.Conv2d(channel_in // 2, channel_out, kernel_size, 1, kernel_size // 2)\n",
    "        self.bn2 = nn.BatchNorm2d(channel_out, eps=1e-4)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 1, kernel_size // 2)\n",
    "\n",
    "        self.up_nn = nn.Upsample(scale_factor=scale_factor, mode=\"nearest\")\n",
    "\n",
    "        self.act_fnc = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up_nn(x)\n",
    "        skip = self.conv3(x)\n",
    "        x = self.act_fnc(self.bn1(self.conv1(x)))\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return self.act_fnc(self.bn2(x + skip))\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channel_in, channel_out, kernel_size=3, act=nn.ELU()):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channel_in, channel_in // 2, kernel_size, 1, kernel_size // 2)\n",
    "        self.bn1 = nn.BatchNorm2d(channel_in // 2, eps=1e-4)\n",
    "        self.conv2 = nn.Conv2d(channel_in // 2, channel_out, kernel_size, 1, kernel_size // 2)\n",
    "        self.bn2 = nn.BatchNorm2d(channel_out, eps=1e-4)\n",
    "\n",
    "        if not channel_in == channel_out:\n",
    "            self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 1, kernel_size // 2)\n",
    "        else:\n",
    "            self.conv3 = nn.Identity()\n",
    "\n",
    "        self.act_fnc = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip = self.conv3(x)\n",
    "        x = self.act_fnc(self.bn1(self.conv1(x)))\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return self.act_fnc(self.bn2(x+skip))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, ch=64, blocks=(1, 2, 4, 8), latent_channels=512):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv_in = nn.Conv2d(channels, blocks[0] * ch, 3, 1, 1)\n",
    "\n",
    "        widths_in = list(blocks)\n",
    "        widths_out = list(blocks[1:]) + [blocks[-1]]\n",
    "\n",
    "        layer_blocks = []\n",
    "\n",
    "        for w_in, w_out in zip(widths_in, widths_out):\n",
    "            layer_blocks.append(ResDown(w_in * ch, w_out * ch))\n",
    "\n",
    "        layer_blocks.append(ResBlock(blocks[-1] * ch, blocks[-1] * ch, kernel_size=3, act=nn.SiLU()))\n",
    "        layer_blocks.append(ResBlock(blocks[-1] * ch, blocks[-1] * ch, kernel_size=3, act=nn.SiLU()))\n",
    "\n",
    "        self.res_blocks = nn.Sequential(*layer_blocks)\n",
    "\n",
    "        self.conv_mu = nn.Conv2d(blocks[-1] * ch, latent_channels, 1, 1)\n",
    "        self.conv_log_var = nn.Conv2d(blocks[-1] * ch, latent_channels, 1, 1)\n",
    "        self.act_fnc = nn.SiLU()\n",
    "\n",
    "    def sample(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, sample=False):\n",
    "        x = self.act_fnc(self.conv_in(x))\n",
    "        x = self.res_blocks(x)\n",
    "\n",
    "        mu = self.conv_mu(x)\n",
    "        log_var = self.conv_log_var(x)\n",
    "\n",
    "\n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder block\n",
    "    Built to be a mirror of the encoder block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, ch=64, blocks=(1, 2, 4, 8), latent_channels=512):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv_in = nn.Conv2d(latent_channels, ch * blocks[-1], 1, 1)\n",
    "\n",
    "        widths_out = list(blocks)[::-1]\n",
    "        widths_in = (list(blocks[1:]) + [blocks[-1]])[::-1]\n",
    "\n",
    "        layer_blocks = [ResBlock(blocks[-1] * ch, blocks[-1] * ch, kernel_size=3, act=nn.ELU()),\n",
    "                        ResBlock(blocks[-1] * ch, blocks[-1] * ch, kernel_size=3, act=nn.ELU())]\n",
    "\n",
    "        for w_in, w_out in zip(widths_in, widths_out):\n",
    "            layer_blocks.append(ResUp(w_in * ch, w_out * ch))\n",
    "\n",
    "        self.res_blocks = nn.Sequential(*layer_blocks)\n",
    "\n",
    "        self.conv_out = nn.Conv2d(blocks[0] * ch, channels, 3, 1, 1)\n",
    "        self.act_fnc = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act_fnc(self.conv_in(x))\n",
    "        x = self.res_blocks(x)\n",
    "        mu = torch.tanh(self.conv_out(x))\n",
    "        return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e790ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAMPVAE(nn.Module):\n",
    "    # Constructor remains the same\n",
    "    def __init__(self, input_channels, latent_channels=512, ch=64, blocks=(1, 2, 4, 8), number_components=10):\n",
    "        super(VAMPVAE, self).__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.latent_channels = latent_channels\n",
    "        # Assuming the input size for the encoder is 192x192\n",
    "        self.expected_height = 128\n",
    "        self.expected_width = 128\n",
    "        \n",
    "        self.encoder = Encoder(channels=input_channels, ch=ch, blocks=blocks, latent_channels=latent_channels)\n",
    "        self.decoder = Decoder(channels=input_channels, ch=ch, blocks=blocks, latent_channels=latent_channels)\n",
    "        self.number_components = number_components\n",
    "\n",
    "        # Pseudo-inputs network\n",
    "        self.latent_height = 8  # Example value, adjust as needed\n",
    "        self.latent_width = 8   # Example value, adjust as needed\n",
    "        self.pseudo_input_height = 128  # Adjust based on your model architecture\n",
    "        self.pseudo_input_width = 128   # Adjust based on your model architecture\n",
    "\n",
    "        self.pseudo_inputs = nn.Sequential(\n",
    "            nn.Linear(number_components, 3*self.pseudo_input_height*self.pseudo_input_width),\n",
    "            nn.Hardtanh(0.0, 1.0)\n",
    "        )\n",
    "\n",
    "        self.idle_input = torch.eye(number_components, requires_grad=False)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, epoch=100):\n",
    "        mu, log_var = self.encoder(x)\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        z, _ = self._sample_gauss(mu, std)\n",
    "\n",
    "        recon_x = self.decoder(z)\n",
    "\n",
    "        loss, recon_loss, kld = self.loss_function(recon_x, x, mu, log_var, z, epoch)\n",
    "\n",
    "        return {\n",
    "            'recon_loss': recon_loss,\n",
    "            'reg_loss': kld,\n",
    "            'loss': loss,\n",
    "            'recon_x': recon_x,\n",
    "            'z': z}\n",
    "\n",
    "    def loss_function(self, recon_x, x, mu, log_var, z, epoch):\n",
    "        # Assuming the reconstruction loss is MSE, modify as needed\n",
    "        recon_loss = F.mse_loss(recon_x, x, reduction='sum') / x.size(0)\n",
    "\n",
    "        log_p_z = self._log_p_z(z)\n",
    "        log_q_z = (-0.5 * (log_var + torch.pow(z - mu, 2) / log_var.exp()+ 1e-4)).sum(dim=[1, 2, 3])\n",
    "    \n",
    "        KLD = -(log_p_z - log_q_z)\n",
    "        \n",
    "\n",
    "        # Linear scheduling for beta\n",
    "        beta = 1.0\n",
    "\n",
    "        return recon_loss + beta * KLD, recon_loss, KLD\n",
    "\n",
    "    def _log_p_z(self, z):\n",
    "        \n",
    "        C = self.number_components\n",
    "        pseudo_inputs = self.pseudo_inputs(self.idle_input.to(z.device))\n",
    "        \n",
    "        pseudo_inputs = pseudo_inputs.view(C, 3, self.pseudo_input_height, self.pseudo_input_height)\n",
    "        \n",
    "\n",
    "        # Pass the pseudo inputs through the encoder\n",
    "        pseudo_mu, pseudo_log_var = self.encoder(pseudo_inputs)\n",
    "\n",
    "        # Calculate the actual latent dimension size per component\n",
    "        total_elements = pseudo_mu.numel()  # Total number of elements in pseudo_mu\n",
    "        latent_dim_per_component = total_elements // C  # Dividing by the number of components\n",
    "\n",
    "        # Flatten pseudo_mu and pseudo_log_var to match this size\n",
    "        pseudo_mu_flat = pseudo_mu.view(C, latent_dim_per_component)\n",
    "        pseudo_log_var_flat = pseudo_log_var.view(C, latent_dim_per_component)\n",
    "\n",
    "        # Flatten z and expand for broadcasting\n",
    "        z_flat = z.view(z.size(0), -1)\n",
    "        z_expand = z_flat.unsqueeze(1)  # Shape: (batch_size, 1, latent_dim_per_component)\n",
    "\n",
    "        # Calculate log probabilities\n",
    "        log_p_z = -0.5 * (pseudo_log_var_flat + (z_expand - pseudo_mu_flat) ** 2 / torch.exp(pseudo_log_var_flat))\n",
    "        log_p_z = log_p_z.sum(dim=2) - torch.log(torch.tensor(C, dtype=torch.float, device=z.device))\n",
    "        log_p_z = torch.logsumexp(log_p_z, dim=1)\n",
    "\n",
    "\n",
    "        return log_p_z\n",
    "\n",
    "    def _sample_gauss(self, mu, std):\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std, eps\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe40091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae_net = VAMPVAE(input_channels=3, ch=64, blocks=(1, 2, 4, 8), latent_channels=64, number_components=50).to(device)\n",
    "# setup optimizer\n",
    "optimizer = optim.Adam(vae_net.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "#Loss function\n",
    "loss_log = []\n",
    "from torchsummary import summary\n",
    "summary(vae_net, (3,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f1d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(channels=3, ch=64, blocks=(1, 2, 4, 8), latent_channels=64).to(device)\n",
    "#summary(enc, (3,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f410d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d6f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_input = torch.randn(4, 3, 128, 128).cuda()  # Replace with appropriate dimensions\n",
    "reconstructed_output = vae_net(sample_input)['loss']\n",
    "print(reconstructed_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f809827",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = vae_net(sample_input)['loss']\n",
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b89c132",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nepoch = 300\n",
    "\n",
    "                \n",
    "for epoch in trange(start_epoch, nepoch, leave=False):\n",
    "    train_loss = 0\n",
    "    train_recon_loss = 0\n",
    "    train_kld_loss = 0\n",
    "    \n",
    "    vae_net.train()\n",
    "    for i, images in enumerate(tqdm(train_loader, leave=False)):\n",
    "        images = images.to(device)\n",
    "\n",
    "        recon_img = vae_net(images)['recon_x']\n",
    "        #VAE loss\n",
    "        kl_loss_ = vae_net(images)['reg_loss'].mean()\n",
    "        mse_loss = vae_net(images)['recon_loss']\n",
    "\n",
    "        loss = vae_net(images)['loss'].mean()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_recon_loss += mse_loss.item()\n",
    "        train_kld_loss += kl_loss_.item()\n",
    "        #train_perceptual_loss += feature_loss.item()\n",
    "        \n",
    "        loss_log.append(loss.item())\n",
    "        vae_net.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(vae_net.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "    avg_loss = train_loss / len(train_loader)  # Calculate average loss for the epoch\n",
    "    avg_recon_loss = train_recon_loss / len(train_loader) \n",
    "    avg_kld_loss = train_kld_loss / len(train_loader) \n",
    "    #avg_perceptual_loss = train_perceptual_loss / len(train_loader)\n",
    "\n",
    "    #In eval mode the model will use mu as the encoding instead of sampling from the distribution\n",
    "    vae_net.eval()\n",
    "    with torch.no_grad():\n",
    "        recon_img = vae_net(test_images.to(device))['recon_x']\n",
    "        img_cat = torch.cat((recon_img.cpu(), test_images), 2)\n",
    "\n",
    "        vutils.save_image(img_cat,\n",
    "                          \"%s/%s/%s_%d.png\" % (save_dir, \"Results\" , model_name, image_size),\n",
    "                          normalize=True)\n",
    "\n",
    "        #Save a checkpoint\n",
    "        torch.save({\n",
    "                    'epoch'                         : epoch,\n",
    "                    'loss_log'                      : loss_log,\n",
    "                    'model_state_dict'              : vae_net.state_dict(),\n",
    "                    'optimizer_state_dict'          : optimizer.state_dict()\n",
    "\n",
    "                     }, save_dir + \"/Models/\" + model_name + \"_\" + str(image_size) + \".pt\")\n",
    "    print(f'Epoch {epoch}/{nepoch} - Avg Total Loss: {avg_loss} - Avg Recon Loss: {avg_recon_loss}\\\n",
    "    - Avg KLD Loss: {avg_kld_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7dcea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
