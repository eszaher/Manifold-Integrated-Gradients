{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "748b3fd0",
      "metadata": {
        "id": "748b3fd0",
        "outputId": "d8144325-bb94-4f8b-fb36-c318be716c8c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.datasets as Datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.utils as vutils\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from torchvision.transforms import functional as TF\n",
        "import PIL\n",
        "\n",
        "batch_size = 64\n",
        "image_size = 128\n",
        "lr = 1e-4\n",
        "nepoch = 100\n",
        "start_epoch = 0\n",
        "\n",
        "\n",
        "dataset_root = \"\"\n",
        "save_dir = os.getcwd()\n",
        "model_name = \"Wasserstein_AE_oxford\"\n",
        "load_checkpoint  = True\n",
        "\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "gpu_indx  = 0\n",
        "device = torch.device(gpu_indx if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3224e00d",
      "metadata": {
        "id": "3224e00d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, data_dir, file_names, transform=None, num_black_images=350, num_white_images=350, image_size=(3, 128, 128) ):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.file_names = file_names\n",
        "        self.image_size = image_size\n",
        "\n",
        "        # Adad placeholders for black images\n",
        "        self.black_image_placeholder = \"<black_image>\"\n",
        "        self.file_names.extend([self.black_image_placeholder] * num_black_images)\n",
        "        self.white_image_placeholder = \"<white_image>\"\n",
        "        self.file_names.extend([self.white_image_placeholder] * num_white_images)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.file_names[idx] == self.black_image_placeholder:\n",
        "            # Create a black image\n",
        "            black_image = torch.zeros(self.image_size)\n",
        "            black_image = TF.to_pil_image(black_image)\n",
        "            return transforms.ToTensor()(black_image) #self.transform(black_image) if self.transform else black_image\n",
        "\n",
        "        if self.file_names[idx] == self.white_image_placeholder:\n",
        "            # Create a black image\n",
        "            white_image = torch.ones(self.image_size)\n",
        "            white_image = TF.to_pil_image(white_image)\n",
        "            return transforms.ToTensor()(white_image) #self.transform(white_image) if self.transform else black_image\n",
        "\n",
        "\n",
        "        img_name = os.path.join(self.data_dir, self.file_names[idx])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(image_size, image_size)),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.Resize(size=(image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "821d290d",
      "metadata": {
        "id": "821d290d"
      },
      "outputs": [],
      "source": [
        "#!pip install scikit-learn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming all your images are in 'data_dir'\n",
        "all_images = [img for img in os.listdir(dataset_root) if img.endswith('.jpg')]  # Adjust for your file type\n",
        "train_images, test_images = train_test_split(all_images, test_size=0.05, random_state=7)\n",
        "\n",
        "train_dataset = CustomImageDataset(data_dir=dataset_root, file_names=train_images, transform=train_transform, num_black_images=500)\n",
        "test_dataset = CustomImageDataset(data_dir=dataset_root, file_names=test_images, transform=test_transform, num_black_images=500)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e952984",
      "metadata": {
        "id": "4e952984"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e158c5ed",
      "metadata": {
        "id": "e158c5ed",
        "outputId": "3b343f00-81eb-43d7-9185-6ae89b988411"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(train_loader)\n",
        "train_images = next(dataiter)\n",
        "train_images.shape\n",
        "\n",
        "test_dataiter = iter(test_loader)\n",
        "test_images = next(test_dataiter)\n",
        "test_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f0a2b54",
      "metadata": {
        "id": "2f0a2b54",
        "outputId": "be7e013b-756e-4efa-f679-9e9a329e6b54"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20,10))\n",
        "out = vutils.make_grid(test_images[0:64], normalize=True)\n",
        "plt.imshow(out.numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c42de30a",
      "metadata": {
        "id": "c42de30a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0c5403b",
      "metadata": {
        "id": "a0c5403b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
        "from dataclasses import field\n",
        "\n",
        "Tensor = TypeVar('torch.tensor')\n",
        "class ResDown(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual down sampling block for the encoder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channel_in, channel_out, kernel_size=3):\n",
        "        super(ResDown, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channel_in, channel_out // 2, kernel_size, 2, kernel_size // 2)\n",
        "        self.bn1 = nn.BatchNorm2d(channel_out // 2, eps=1e-4)\n",
        "        self.conv2 = nn.Conv2d(channel_out // 2, channel_out, kernel_size, 1, kernel_size // 2)\n",
        "        self.bn2 = nn.BatchNorm2d(channel_out, eps=1e-4)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 2, kernel_size // 2)\n",
        "\n",
        "        self.act_fnc = nn.ELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip = self.conv3(x)\n",
        "        x = self.act_fnc(self.bn1(self.conv1(x)))\n",
        "        x = self.conv2(x)\n",
        "        return self.act_fnc(self.bn2(x + skip))\n",
        "\n",
        "\n",
        "class ResUp(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual up sampling block for the decoder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channel_in, channel_out, kernel_size=3, scale_factor=2):\n",
        "        super(ResUp, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(channel_in, channel_in // 2, kernel_size, 1, kernel_size // 2)\n",
        "        self.bn1 = nn.BatchNorm2d(channel_in // 2, eps=1e-4)\n",
        "        self.conv2 = nn.Conv2d(channel_in // 2, channel_out, kernel_size, 1, kernel_size // 2)\n",
        "        self.bn2 = nn.BatchNorm2d(channel_out, eps=1e-4)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 1, kernel_size // 2)\n",
        "\n",
        "        self.up_nn = nn.Upsample(scale_factor=scale_factor, mode=\"nearest\")\n",
        "\n",
        "        self.act_fnc = nn.ELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up_nn(x)\n",
        "        skip = self.conv3(x)\n",
        "        x = self.act_fnc(self.bn1(self.conv1(x)))\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        return self.act_fnc(self.bn2(x + skip))\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channel_in, channel_out, kernel_size=3):\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(channel_in, channel_in // 2, kernel_size, 1, kernel_size // 2)\n",
        "        self.bn1 = nn.BatchNorm2d(channel_in // 2, eps=1e-4)\n",
        "        self.conv2 = nn.Conv2d(channel_in // 2, channel_out, kernel_size, 1, kernel_size // 2)\n",
        "        self.bn2 = nn.BatchNorm2d(channel_out, eps=1e-4)\n",
        "\n",
        "        if not channel_in == channel_out:\n",
        "            self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 1, kernel_size // 2)\n",
        "        else:\n",
        "            self.conv3 = nn.Identity()\n",
        "\n",
        "        self.act_fnc = nn.ELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip = self.conv3(x)\n",
        "        x = self.act_fnc(self.bn1(self.conv1(x)))\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        return self.act_fnc(self.bn2(x + skip))\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, ch=64, blocks=(1, 2, 4, 8), latent_channels=512):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv_in = nn.Conv2d(channels, blocks[0] * ch, 3, 1, 1)\n",
        "\n",
        "        widths_in = list(blocks)\n",
        "        widths_out = list(blocks[1:]) + [blocks[-1]]\n",
        "\n",
        "        layer_blocks = []\n",
        "\n",
        "        for w_in, w_out in zip(widths_in, widths_out):\n",
        "            layer_blocks.append(ResDown(w_in * ch, w_out * ch))\n",
        "\n",
        "        layer_blocks.append(ResBlock(blocks[-1] * ch, blocks[-1] * ch))\n",
        "        layer_blocks.append(ResBlock(blocks[-1] * ch, blocks[-1] * ch))\n",
        "\n",
        "        self.res_blocks = nn.Sequential(*layer_blocks)\n",
        "\n",
        "        self.conv_mu = nn.Conv2d(blocks[-1] * ch, latent_channels, 1, 1)\n",
        "        self.act_fnc = nn.ELU()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, sample=False):\n",
        "        x = self.act_fnc(self.conv_in(x))\n",
        "        x = self.res_blocks(x)\n",
        "        z = self.conv_mu(x)\n",
        "        return z\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder block\n",
        "    Built to be a mirror of the encoder block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, ch=64, blocks=(1, 2, 4, 8), latent_channels=512):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv_in = nn.Conv2d(latent_channels, ch * blocks[-1], 1, 1)\n",
        "\n",
        "        widths_out = list(blocks)[::-1]\n",
        "        widths_in = (list(blocks[1:]) + [blocks[-1]])[::-1]\n",
        "\n",
        "        layer_blocks = [ResBlock(blocks[-1] * ch, blocks[-1] * ch),\n",
        "                        ResBlock(blocks[-1] * ch, blocks[-1] * ch)]\n",
        "\n",
        "        for w_in, w_out in zip(widths_in, widths_out):\n",
        "            layer_blocks.append(ResUp(w_in * ch, w_out * ch))\n",
        "\n",
        "        self.res_blocks = nn.Sequential(*layer_blocks)\n",
        "\n",
        "        self.conv_out = nn.Conv2d(blocks[0] * ch, channels, 3, 1, 1)\n",
        "        self.act_fnc = nn.ELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act_fnc(self.conv_in(x))\n",
        "        x = self.res_blocks(x)\n",
        "        mu = torch.tanh(self.conv_out(x))\n",
        "        return mu\n",
        "\n",
        "class WassersteinAE(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 channel_in: int = 3,\n",
        "                 ch=64,\n",
        "                 blocks=(1, 2, 4, 8),\n",
        "                 latent_channels=512,\n",
        "                 latent_dim = 4096,\n",
        "                 kernel_bandwidth = 1.0,\n",
        "                 kernel_choice = \"imq\",\n",
        "                 scales = [0.1, 0.2, 0.5, 1.0, 2.0, 5, 10.0],\n",
        "                 reconstruction_loss_scale = 1.0,\n",
        "                 reg_weight = 3e-2\n",
        "                 ):\n",
        "        super(WassersteinAE, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.kernel_bandwidth = kernel_bandwidth\n",
        "\n",
        "        self.kernel_choice = kernel_choice\n",
        "        self.scales = scales #model_config.scales if model_config.scales is not None else [1.0]\n",
        "        self.reconstruction_loss_scale = reconstruction_loss_scale #self.model_config.reconstruction_loss_scale\n",
        "        self.reg_weight = reg_weight\n",
        "\n",
        "\n",
        "        self.encoder = Encoder(channel_in, ch=ch, blocks=blocks, latent_channels=latent_channels)\n",
        "        self.decoder = Decoder(channel_in, ch=ch, blocks=blocks, latent_channels=latent_channels)\n",
        "\n",
        "\n",
        "\n",
        "    def encode(self, input: Tensor) -> Tensor:\n",
        "        result = self.encoder(input)\n",
        "        #print(\"After encoder:\", result.shape)\n",
        "        #result = torch.flatten(result, start_dim=1)\n",
        "        #print(\"After flattening:\", result.shape)\n",
        "        #encoded = self.fc_encode(result)\n",
        "        #print(\"After linear layer:\", encoded.shape)\n",
        "        return result\n",
        "\n",
        "    def decode(self, z: Tensor) -> Tensor:\n",
        "        #result = self.decoder_input(z)\n",
        "        #print(\"After linear (decoder input):\", result.shape)\n",
        "\n",
        "        #result = result.view(-1, 512, 2, 2)\n",
        "        #print(\"After reshaping:\", result.shape)\n",
        "\n",
        "        result = self.decoder(z)\n",
        "        #print(\"After decoder:\", result.shape)\n",
        "        #result = self.final_upsample(result)\n",
        "\n",
        "        # Apply the final convolutional layer\n",
        "        #result = self.final_layer(result)\n",
        "\n",
        "        #print(\"After final layer:\", result.shape)\n",
        "\n",
        "\n",
        "        return result\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        z = self.encode(x)\n",
        "        recon_x = self.decode(z)\n",
        "        z_prior = torch.randn_like(z, device=x.device)\n",
        "        loss, recon_loss, mmd_loss = self.loss_function(recon_x, x, z, z_prior)\n",
        "\n",
        "        return {\n",
        "            'recon_loss': recon_loss,\n",
        "            'mmd_loss': mmd_loss,\n",
        "            'loss': loss,\n",
        "            'recon_x': recon_x,\n",
        "            'z': z}\n",
        "\n",
        "\n",
        "    def loss_function(self, recon_x, x, z, z_prior):\n",
        "        N = z.shape[0]  # batch size\n",
        "\n",
        "        recon_loss = self.reconstruction_loss_scale * F.mse_loss(\n",
        "            recon_x.reshape(x.shape[0], -1), x.reshape(x.shape[0], -1), reduction=\"none\"\n",
        "        ).sum(dim=-1)\n",
        "\n",
        "\n",
        "        if self.kernel_choice == \"rbf\":\n",
        "            k_z = self.rbf_kernel(z, z)\n",
        "            k_z_prior = self.rbf_kernel(z_prior, z_prior)\n",
        "            k_cross = self.rbf_kernel(z, z_prior)\n",
        "\n",
        "        else:\n",
        "            k_z = self.imq_kernel(z, z)\n",
        "            k_z_prior = self.imq_kernel(z_prior, z_prior)\n",
        "            k_cross = self.imq_kernel(z, z_prior)\n",
        "\n",
        "        mmd_z = (k_z - k_z.diag().diag()).sum() / ((N - 1) * N)\n",
        "        mmd_z_prior = (k_z_prior - k_z_prior.diag().diag()).sum() / ((N - 1) * N)\n",
        "        mmd_cross = k_cross.sum() / (N**2)\n",
        "\n",
        "        mmd_loss = mmd_z + mmd_z_prior - 2 * mmd_cross\n",
        "\n",
        "        return (\n",
        "            recon_loss.mean(dim=0) + self.reg_weight * mmd_loss,\n",
        "            (recon_loss).mean(dim=0),\n",
        "            mmd_loss,\n",
        "        )\n",
        "    def imq_kernel(self, z1, z2):\n",
        "        \"\"\"Returns a matrix of shape [batch x batch] containing the pairwise kernel computation\"\"\"\n",
        "        z1 = z1.view(z1.shape[0], -1)\n",
        "        z2 = z2.view(z2.shape[0], -1)\n",
        "\n",
        "        Cbase = (\n",
        "            2.0 * self.latent_dim * self.kernel_bandwidth**2\n",
        "        )\n",
        "\n",
        "        k = 0\n",
        "\n",
        "        for scale in self.scales:\n",
        "            C = scale * Cbase\n",
        "            k += C / (C + torch.norm(z1.unsqueeze(1) - z2.unsqueeze(0), dim=-1) ** 2)\n",
        "\n",
        "        return k\n",
        "\n",
        "    def rbf_kernel(self, z1, z2):\n",
        "        \"\"\"Returns a matrix of shape [batch x batch] containing the pairwise kernel computation\"\"\"\n",
        "        z1 = z1.view(z1.shape[0], -1)\n",
        "        z2 = z2.view(z2.shape[0], -1)\n",
        "\n",
        "        C = 2.0 * self.latent_dim * self.kernel_bandwidth**2\n",
        "\n",
        "        k = torch.exp(-torch.norm(z1.unsqueeze(1) - z2.unsqueeze(0), dim=-1) ** 2 / C)\n",
        "\n",
        "        return k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e8962f",
      "metadata": {
        "id": "e3e8962f",
        "outputId": "d2057169-5cbd-4784-c175-2396022e27f1"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "import os\n",
        "\n",
        "AE_net = WassersteinAE(channel_in=3, ch=64, blocks=(1, 2, 4, 8), latent_channels=64).to(device)\n",
        "optimizer = optim.Adam(AE_net.parameters(), lr=lr)\n",
        "#scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = scheduler_gamma)\n",
        "summary(AE_net, (3, 128, 128))\n",
        "loss_log = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e091d69",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "1adbcd477e1c4703b7e91919134a62e1",
            "",
            "4dda05b7dbe940e98e62ee593694e9d2"
          ]
        },
        "id": "1e091d69",
        "outputId": "6c7d2008-92cf-48ea-e9c1-6dd9fae62e0a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "nepoch = 250\n",
        "\n",
        "\n",
        "for epoch in trange(start_epoch, nepoch, leave=False):\n",
        "    train_loss = 0\n",
        "    train_recon_loss = 0\n",
        "    train_kld_loss = 0\n",
        "\n",
        "    AE_net.train()\n",
        "    for i, images in enumerate(tqdm(train_loader, leave=False)):\n",
        "        images = images.to(device)\n",
        "\n",
        "        recon_img = AE_net(images)['recon_x']\n",
        "        #VAE loss\n",
        "        kl_loss_ = AE_net(images)['mmd_loss']\n",
        "        mse_loss = AE_net(images)['recon_loss']\n",
        "\n",
        "        loss = AE_net(images)['loss']\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_recon_loss += mse_loss.item()\n",
        "        train_kld_loss += kl_loss_.item()\n",
        "        #train_perceptual_loss += feature_loss.item()\n",
        "\n",
        "        loss_log.append(loss.item())\n",
        "        AE_net.zero_grad()\n",
        "        loss.backward()\n",
        "        #torch.nn.utils.clip_grad_norm_(AE_net.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = train_loss / len(train_loader)  # Calculate average loss for the epoch\n",
        "    avg_recon_loss = train_recon_loss / len(train_loader)\n",
        "    avg_kld_loss = train_kld_loss / len(train_loader)\n",
        "    #avg_perceptual_loss = train_perceptual_loss / len(train_loader)\n",
        "\n",
        "    #In eval mode the model will use mu as the encoding instead of sampling from the distribution\n",
        "    AE_net.eval()\n",
        "    with torch.no_grad():\n",
        "        recon_img = AE_net(test_images.to(device))['recon_x']\n",
        "        img_cat = torch.cat((recon_img.cpu(), test_images), 2)\n",
        "\n",
        "        vutils.save_image(img_cat,\n",
        "                          \"%s/%s/%s_%d.png\" % (save_dir, \"Results\" , model_name, image_size),\n",
        "                          normalize=True)\n",
        "\n",
        "        #Save a checkpoint\n",
        "        torch.save({\n",
        "                    'epoch'                         : epoch,\n",
        "                    'loss_log'                      : loss_log,\n",
        "                    'model_state_dict'              : AE_net.state_dict(),\n",
        "                    'optimizer_state_dict'          : optimizer.state_dict()\n",
        "\n",
        "                     }, save_dir + \"/Models/\" + model_name + \"_\" + str(image_size) + \".pt\")\n",
        "    print(f'Epoch {epoch}/{nepoch} - Avg Total Loss: {avg_loss} - Avg Recon Loss: {avg_recon_loss}\\\n",
        "    - Avg KLD Loss: {avg_kld_loss}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3283827c",
      "metadata": {
        "id": "3283827c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd673b60",
      "metadata": {
        "id": "cd673b60",
        "outputId": "b5e1ec65-cfe8-4fa8-e03a-97420ffe9d51"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Create the save directory if it does note exist\n",
        "if not os.path.isdir(save_dir + \"/Models\"):\n",
        "    os.makedirs(save_dir + \"/Models\")\n",
        "if not os.path.isdir(save_dir + \"/Results\"):\n",
        "    os.makedirs(save_dir + \"/Results\")\n",
        "\n",
        "if load_checkpoint:\n",
        "    checkpoint = torch.load(save_dir + \"/Models/\" + model_name + \"_\" + str(image_size) + \".pt\", map_location = \"cpu\")\n",
        "    print(\"Checkpoint loaded\")\n",
        "    #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    AE_net.load_state_dict(checkpoint['model_state_dict'])\n",
        "    start_epoch = checkpoint[\"epoch\"]\n",
        "    loss_log = checkpoint[\"loss_log\"]\n",
        "else:\n",
        "    #If checkpoint does exist raise an error to prevent accidental overwriting\n",
        "    if os.path.isfile(save_dir + \"/Models/\" + model_name + \"_\" + str(image_size) + \".pt\"):\n",
        "        raise ValueError(\"Warning Checkpoint exists\")\n",
        "    else:\n",
        "        print(\"Starting from scratch\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58443c52",
      "metadata": {
        "id": "58443c52",
        "outputId": "70c4c2d8-45c9-4a90-c76d-e34e78023267"
      },
      "outputs": [],
      "source": [
        "AE_net.eval()\n",
        "\n",
        "black_image = torch.zeros((3,128,128))\n",
        "black_image = TF.to_pil_image(black_image)\n",
        "\n",
        "\n",
        "#black_image = black_image.unsqueeze(0)\n",
        "#black_image = torch.FloatTensor(1,3,128,128).zero_().cuda()\n",
        "black_image = test_transform(black_image)\n",
        "black_image = black_image.unsqueeze(0).cuda()\n",
        "black_z = AE_net.encoder(black_image)\n",
        "bl = AE_net.decoder(black_z)\n",
        "#black_z,_,_ = vae_net.encoder(bl.cuda())\n",
        "#bl = vae_net.decoder(black_z)\n",
        "out = vutils.make_grid(bl, normalize=False)\n",
        "plt.imshow(out.cpu().detach().numpy().transpose((1, 2, 0)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83ab835b",
      "metadata": {
        "id": "83ab835b",
        "outputId": "d433a879-0c99-42f2-f39c-a331258de887"
      },
      "outputs": [],
      "source": [
        "white_image = torch.ones((3,128,128))\n",
        "white_image = TF.to_pil_image(white_image)\n",
        "\n",
        "#black_image = black_image.unsqueeze(0)\n",
        "#black_image = torch.FloatTensor(1,3,128,128).zero_().cuda()\n",
        "white_image = test_transform(white_image)\n",
        "white_image = white_image.unsqueeze(0).cuda()\n",
        "white_z = AE_net.encoder(white_image.cuda())\n",
        "wl = AE_net.decoder(white_z)\n",
        "#black_z,_,_ = vae_net.encoder(black_image.cuda())\n",
        "#bl = vae_net.decoder(black_z)\n",
        "\n",
        "\n",
        "out = vutils.make_grid(wl, normalize=True)\n",
        "plt.imshow(out.cpu().detach().numpy().transpose((1, 2, 0)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
