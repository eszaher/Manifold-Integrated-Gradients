{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e1b86da",
      "metadata": {
        "id": "2e1b86da"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as  np\n",
        "\n",
        "\n",
        "class CNN_Encoder(nn.Module):\n",
        "    def __init__(self, output_size, input_size=(1, 28, 28)):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.channel_mult = 16\n",
        "\n",
        "        #convolutions\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1,\n",
        "                     out_channels=self.channel_mult*1,\n",
        "                     kernel_size=4,\n",
        "                     stride=1,\n",
        "                     padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(self.channel_mult*1, self.channel_mult*2, 4, 2, 1),\n",
        "            nn.BatchNorm2d(self.channel_mult*2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(self.channel_mult*2, self.channel_mult*4, 4, 2, 1),\n",
        "            nn.BatchNorm2d(self.channel_mult*4),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(self.channel_mult*4, self.channel_mult*8, 4, 2, 1),\n",
        "            nn.BatchNorm2d(self.channel_mult*8),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(self.channel_mult*8, self.channel_mult*16, 3, 2, 1),\n",
        "            nn.BatchNorm2d(self.channel_mult*16),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "\n",
        "        self.flat_fts = self.get_flat_fts(self.conv)\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(self.flat_fts, output_size),\n",
        "            nn.BatchNorm1d(output_size),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "\n",
        "    def get_flat_fts(self, fts):\n",
        "        f = fts(Variable(torch.ones(1, *self.input_size)))\n",
        "        return int(np.prod(f.size()[1:]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x.view(-1, *self.input_size))\n",
        "        x = x.view(-1, self.flat_fts)\n",
        "        return self.linear(x)\n",
        "\n",
        "class CNN_Decoder(nn.Module):\n",
        "    def __init__(self, embedding_size, input_size=(1, 28, 28)):\n",
        "        super(CNN_Decoder, self).__init__()\n",
        "        self.input_height = 28\n",
        "        self.input_width = 28\n",
        "        self.input_dim = embedding_size\n",
        "        self.channel_mult = 16\n",
        "        self.output_channels = 1\n",
        "        self.fc_output_dim = 512\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, self.fc_output_dim),\n",
        "            nn.BatchNorm1d(self.fc_output_dim),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "        self.deconv = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d(self.fc_output_dim, self.channel_mult*4,\n",
        "                                4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(self.channel_mult*4),\n",
        "            nn.ELU(),\n",
        "            # state size. self.channel_mult*32 x 4 x 4\n",
        "            nn.ConvTranspose2d(self.channel_mult*4, self.channel_mult*2,\n",
        "                                3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.channel_mult*2),\n",
        "            nn.ELU(),\n",
        "            # state size. self.channel_mult*16 x 7 x 7\n",
        "            nn.ConvTranspose2d(self.channel_mult*2, self.channel_mult*1,\n",
        "                                4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.channel_mult*1),\n",
        "            nn.ELU(),\n",
        "            # state size. self.channel_mult*8 x 14 x 14\n",
        "            nn.ConvTranspose2d(self.channel_mult*1, self.output_channels, 4, 2, 1, bias=False),\n",
        "            nn.Sigmoid()\n",
        "            # state size. self.output_channels x 28 x 28\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(-1, self.fc_output_dim, 1, 1)\n",
        "        x = self.deconv(x)\n",
        "        return x.view(-1, self.input_width*self.input_height)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39b826db",
      "metadata": {
        "id": "39b826db"
      },
      "outputs": [],
      "source": [
        "class VAE_MNIST(nn.Module):\n",
        "    def __init__(self, output_size=512, embedding_size=16):\n",
        "        super(VAE_MNIST, self).__init__()\n",
        "        output_size = 512\n",
        "        self.encoder = CNN_Encoder(output_size)\n",
        "        self.var = nn.Linear(output_size, embedding_size)\n",
        "        self.mu = nn.Linear(output_size, embedding_size)\n",
        "\n",
        "        self.decoder = CNN_Decoder(embedding_size)\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        mu = self.mu(x)\n",
        "        var = self.var(x)\n",
        "        if self.training:\n",
        "            z = self.reparameterize(mu, var)\n",
        "        else:\n",
        "            z = mu\n",
        "        return z, mu, var\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std).add_(mu)\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, mu, logvar = self.encode(x.view(-1, 784))\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "    def loss_function(self, recon_x, x, mu, logvar):\n",
        "        BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "        # see Appendix B from VAE paper:\n",
        "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "        # https://arxiv.org/abs/1312.6114\n",
        "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        return BCE + KLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d29ac20",
      "metadata": {
        "id": "7d29ac20"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ConvBlock (nn.Sequential):\n",
        "    def __init__ (self, in_c, out_c, kernel_size, stride=1):\n",
        "        super().__init__()\n",
        "        self.add_module('Convolution', nn.utils.spectral_norm(nn.Conv2d(in_c, out_c, kernel_size, stride)))\n",
        "        self.add_module('BatchNorm', nn.BatchNorm2d(out_c, affine=True))\n",
        "        self.add_module('Activation', nn.ELU())\n",
        "\n",
        "class ConvTransposeBlock (nn.Sequential):\n",
        "    def __init__ (self, in_c, out_c, kernel_size, stride=1):\n",
        "        super().__init__()\n",
        "        self.add_module('ConvTranspose', nn.utils.spectral_norm(nn.ConvTranspose2d(in_c, out_c, kernel_size, stride)))\n",
        "        self.add_module('BatchNorm', nn.BatchNorm2d(out_c, affine=True))\n",
        "        self.add_module('Activation', nn.ELU())\n",
        "\n",
        "\n",
        "class Encoder (nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder of 2D VAE, producing latent multivariate normal distributions from input images. We return logarithmic variances as they have the real numbers as domain.\n",
        "\n",
        "    Forward pass:\n",
        "        1 Input:\n",
        "            i)  Image of shape [N, C, H, W] (by default 1x28x28 MNIST images)\n",
        "        2 Outputs:\n",
        "            i)  Means of latent distributions of shape [N, latent_dim]\n",
        "            ii) Logarithmic variances of latent distribution of shape [N, latent_dim] (Approximation of multivariate Gaussian, covariance is strictly diagonal, i.e. [N, d, d] is now [N, d])\n",
        "\n",
        "    Arguments:\n",
        "        X_dim (list) : dimensions of input 2D image, in the form of [Channels, Height, Width]\n",
        "        latent_dim (int) : dimension of latent space.\n",
        "    \"\"\"\n",
        "    def __init__(self, X_dim=[1,28,28], latent_dim=16):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        conv1_outchannels = 32\n",
        "        conv2_outchannels = 32\n",
        "\n",
        "        # How the convolutions change the shape\n",
        "        conv_outputshape = (\n",
        "            conv2_outchannels\n",
        "            * int(((X_dim[1]-4)/2 - 2)/2 + 1)\n",
        "            * int(((X_dim[2]-4)/2 - 2)/2 + 1)\n",
        "        )\n",
        "\n",
        "        self.enc = nn.Sequential(\n",
        "            ConvBlock(X_dim[0], conv1_outchannels, kernel_size=4, stride=2),\n",
        "            ConvBlock(conv1_outchannels, conv2_outchannels, kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        self.zmean = nn.Linear(conv_outputshape, latent_dim)\n",
        "        self.zlogvar = nn.Linear(conv_outputshape, latent_dim)\n",
        "\n",
        "\n",
        "    def forward (self, X):\n",
        "        x = self.enc(X)\n",
        "        x = x.view(X.shape[0], -1)\n",
        "        mean = self.zmean(x)\n",
        "        logvar = self.zlogvar(x)\n",
        "\n",
        "        return mean, logvar\n",
        "\n",
        "\n",
        "class Decoder (nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder of 2D VAE, producing output multivariate normal distributions from latent vectors. We return logarithmic variances as they have the real numbers as domain.\n",
        "\n",
        "    Forward pass:\n",
        "        1 Input:\n",
        "            i)  Latent vector of shape [N, latent_dim]\n",
        "        2 Outputs:\n",
        "            i)  Means of output distributions of shape [N, C, H, W]\n",
        "            ii) Variances of output distribution of shape [N, C, H, W] (Approximation of multivariate Gaussian, covariance is strictly diagonal). We assume constant variance during VAE training.\n",
        "\n",
        "    Arguments:\n",
        "        X_dim (list) : dimensions of input 2D image, in the form of [Channels, Height, Width]\n",
        "        latent_dim (int) : dimension of latent space.\n",
        "    \"\"\"\n",
        "    def __init__(self, X_dim=[1,28,28], latent_dim=16):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # Currently number of clusters is set to 4*latent_dim as a good rule of thumb, can be changed, but then also change it later during training!\n",
        "        #self.improved_variance = True\n",
        "        #k = 4*latent_dim\n",
        "        #self.rbfNN = RBF(centers=pt.zeros(k,latent_dim), bandwidth=pt.zeros(k), X_dim=np.prod(X_dim))\n",
        "\n",
        "        conv1_outchannels = 32\n",
        "        conv2_outchannels = 32\n",
        "\n",
        "        # How the convolutions change the shape\n",
        "        self.conv_outputshape = (\n",
        "            int(((X_dim[1]-4)/2 - 2)/2 + 1),\n",
        "            int(((X_dim[2]-4)/2 - 2)/2 + 1)\n",
        "        )\n",
        "\n",
        "        self.lin = nn.Linear(latent_dim, conv2_outchannels*self.conv_outputshape[0]*self.conv_outputshape[1])\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            ConvTransposeBlock(conv2_outchannels, conv1_outchannels, kernel_size=3, stride=2),\n",
        "            ConvTransposeBlock(conv1_outchannels, 32, kernel_size=4, stride=2)\n",
        "        )\n",
        "\n",
        "        self.Xmean = nn.Sequential(\n",
        "            nn.Conv2d(32, X_dim[0], kernel_size=1),\n",
        "            # Output is grayscale between -1 and 1\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward (self, z):\n",
        "        \"\"\"\n",
        "        When improved_variance is set to True, we use a trained RBF to return a better variance estimate as described in \"Arvanitidis et al. (2018): Latent Space Oddity\". Of course the RBF has to be assigned to the Decoder first.\n",
        "        \"\"\"\n",
        "        x = self.lin(z)\n",
        "        x = x.view(z.shape[0], -1, self.conv_outputshape[0], self.conv_outputshape[1])\n",
        "        x = self.conv(x)\n",
        "        mean = self.Xmean(x)\n",
        "\n",
        "        #if not self.improved_variance:\n",
        "            # We freeze the variance as constant 0.5. Requires grad so metric computation goes smoothly (i.e. returns no gradient)\n",
        "        #    var = pt.ones_like(mean, requires_grad=True) * 0.5\n",
        "        #else:\n",
        "        #    var = 1/self.rbfNN(z)\n",
        "\n",
        "        return mean#, var\n",
        "\n",
        "class VAE_MNIST(nn.Module):\n",
        "    def __init__(self, output_size=512, embedding_size=16):\n",
        "        super(VAE_MNIST, self).__init__()\n",
        "        output_size = 512\n",
        "        self.encoder = Encoder()\n",
        "        #self.var = nn.Linear(output_size, embedding_size)\n",
        "        #self.mu = nn.Linear(output_size, embedding_size)\n",
        "\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def encode(self, x):\n",
        "        mu, var = self.encoder(x)\n",
        "        #mu = self.mu(x)\n",
        "        #var = self.var(x)\n",
        "        if self.training:\n",
        "            z = self.reparameterize(mu, var)\n",
        "        else:\n",
        "            z = mu\n",
        "        return z, mu, var\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std).add_(mu)\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, mu, logvar = self.encode(x.view(-1, 784))\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "    def loss_function(self, recon_x, x, mu, logvar):\n",
        "        BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "        # see Appendix B from VAE paper:\n",
        "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "        # https://arxiv.org/abs/1312.6114\n",
        "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        return BCE + KLD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "695ae625",
      "metadata": {
        "id": "695ae625"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.datasets as Datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.utils as vutils\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "latent_channels = 16\n",
        "batch_size = 128\n",
        "lr = 0.005\n",
        "nepoch = 20\n",
        "start_epoch = 0\n",
        "dataset_root = \"\"\n",
        "save_dir = os.getcwd()\n",
        "model_name = \"MNIST_VAE__black_lrelu\" + \"_\" + str(latent_channels)\n",
        "load_checkpoint  = False\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "gpu_indx  = 0\n",
        "device = torch.device(gpu_indx if use_cuda else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f43047d",
      "metadata": {
        "id": "7f43047d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import numpy as np\n",
        "\n",
        "class BlackImagesDataset(Dataset):\n",
        "    def __init__(self, num_images, image_size, label):\n",
        "        self.num_images = num_images\n",
        "        self.image_size = image_size\n",
        "        self.label = label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_images\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = torch.zeros(self.image_size, dtype=torch.float32)\n",
        "        label = self.label\n",
        "        return image, label\n",
        "\n",
        "class WhiteImagesDataset(Dataset):\n",
        "    def __init__(self, num_images, image_size, label):\n",
        "        self.num_images = num_images\n",
        "        self.image_size = image_size\n",
        "        self.label = label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_images\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = torch.ones(self.image_size, dtype=torch.float32)\n",
        "        label = self.label\n",
        "        return image, label\n",
        "\n",
        "def prepare_datasets(num_black_images=50):\n",
        "    # Load MNIST dataset\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Create dataset of black images (label 10 for black images)\n",
        "    black_train_dataset = BlackImagesDataset(num_black_images, (1, 28, 28), 10)\n",
        "    black_test_dataset = BlackImagesDataset(num_black_images, (1, 28, 28), 10)\n",
        "\n",
        "    white_train_dataset = WhiteImagesDataset(num_black_images, (1, 28, 28), 11)\n",
        "    white_test_dataset = WhiteImagesDataset(num_black_images, (1, 28, 28), 11)\n",
        "\n",
        "    # Concatenate MNIST with black images dataset\n",
        "    train_dataset = ConcatDataset([train_dataset, black_train_dataset, white_train_dataset])\n",
        "    test_dataset = ConcatDataset([test_dataset, black_test_dataset, white_test_dataset])\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "\n",
        "\n",
        "# Prepare datasets\n",
        "train_dataset, test_dataset = prepare_datasets()\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "# Now you can use train_loader and test_loader for training and testing your VAE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99d32fa1",
      "metadata": {
        "id": "99d32fa1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e27386be",
      "metadata": {
        "id": "e27386be",
        "outputId": "75d30289-928d-48b2-d33b-5c18030de585"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "882a9f9b",
      "metadata": {
        "id": "882a9f9b"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "# Define a transform to normalize the data\n",
        "save_dir = os.getcwd()\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "# Download and load the training data\n",
        "trainset = torchvision.datasets.MNIST(save_dir, download=True, train=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "testset = torchvision.datasets.MNIST(save_dir, download=True, train=False, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1492992",
      "metadata": {
        "id": "d1492992"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(test_loader)\n",
        "test_images, test_labels = next(dataiter)\n",
        "test_images[0]\n",
        "train_iter = iter(train_loader)\n",
        "train_images, train_labels = next(train_iter)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92d676a6",
      "metadata": {
        "id": "92d676a6",
        "outputId": "a52edafd-30d1-4132-9d35-209934de5387"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (5,10))\n",
        "out = vutils.make_grid(test_images[0:], normalize=True)\n",
        "plt.imshow(out.numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "593ca2f9",
      "metadata": {
        "id": "593ca2f9",
        "outputId": "8921e974-c88c-4781-c5e0-2edee2d89454"
      },
      "outputs": [],
      "source": [
        "test_labels[2].dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "167e79ad",
      "metadata": {
        "id": "167e79ad",
        "outputId": "03d9f669-89ab-453f-d0c4-77709476de07"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "vae_net = VAE_MNIST().to(device)\n",
        "\n",
        "optimizer = optim.Adam(vae_net.parameters(), lr=lr, weight_decay=1e-4)\n",
        "#Loss function\n",
        "loss_log = []\n",
        "from torchsummary import summary\n",
        "summary(vae_net, (1, 28, 28))\n",
        "vae_net.eval()\n",
        "recon_img, mu, logvar = vae_net(test_images[0].view(-1,1,28,28).to(device))\n",
        "recon_img.shape\n",
        "recon_img.view(1,1,28,28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82b8caee",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            ""
          ]
        },
        "id": "82b8caee",
        "outputId": "6fd251c2-e5b1-427f-b7a6-b9995a253c9c",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "\n",
        "for epoch in trange(start_epoch, num_epochs, leave=False):\n",
        "    vae_net.train()\n",
        "    train_loss = 0\n",
        "    for i, (images, _) in enumerate(tqdm(train_loader, leave=False)):\n",
        "        images = images.to(device)\n",
        "\n",
        "        recon_img, mu, logvar = vae_net(images)\n",
        "\n",
        "        loss = vae_net.loss_function(recon_img, images, mu, logvar)\n",
        "\n",
        "        vae_net.zero_grad()\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = train_loss / len(train_loader)  # Calculate average loss for the epoch\n",
        "    loss_log.append(avg_loss)  # Append average loss to loss_log\n",
        "    #In eval mode the model will use mu as the encoding instead of sampling from the distribution\n",
        "    vae_net.eval()\n",
        "    with torch.no_grad():\n",
        "        recon_img, _, _ = vae_net(test_images.to(device))\n",
        "        img_cat = torch.cat((recon_img.view(-1,1,28,28).cpu(), test_images), 2)\n",
        "\n",
        "        vutils.save_image(img_cat,\n",
        "                          \"%s/%s/%s_%d.png\" % (save_dir, \"Results\" , model_name, 28),\n",
        "                          normalize=True)\n",
        "\n",
        "        #Save a checkpoint\n",
        "        torch.save({\n",
        "                    'epoch'                         : epoch,\n",
        "                    'loss_log'                      : loss_log,\n",
        "                    'model_state_dict'              : vae_net.state_dict(),\n",
        "                    'optimizer_state_dict'          : optimizer.state_dict()\n",
        "\n",
        "                     }, save_dir + \"/Models/\" + model_name  + \".pt\")\n",
        "    print(f'Epoch {epoch}/{num_epochs} - Avg Loss: {avg_loss}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e560c263",
      "metadata": {
        "id": "e560c263",
        "outputId": "0eae74da-cdbd-41aa-ad74-6aec5d2cd6bd",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "gpu_indx  = 0\n",
        "device = torch.device(gpu_indx if use_cuda else \"cpu\")\n",
        "image_size = 28\n",
        "\n",
        "vae_net = VAE_MNIST().to(device)\n",
        "\n",
        "# setup optimizer\n",
        "\n",
        "checkpoint = torch.load(save_dir + \"/Models/\" + model_name + \".pt\")\n",
        "print(\"Checkpoint loaded\")\n",
        "vae_net.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "vae_net.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22e68760",
      "metadata": {
        "id": "22e68760"
      },
      "outputs": [],
      "source": [
        "image_size = 28\n",
        "vae_net.eval()\n",
        "def compute_etta(model, zi, zi_minus, zi_plus, dt):\n",
        "    # Compute the finite difference\n",
        "    g_zi_minus = model.decode(zi_minus).view(-1)\n",
        "    g_zi = model.decode(zi).view(-1)\n",
        "    g_zi_plus = model.decode(zi_plus).view(-1)\n",
        "\n",
        "    finite_diff = (g_zi_plus - 2 * g_zi + g_zi_minus) / dt\n",
        "    finite_diff = finite_diff.view(1, 1, image_size, image_size)  # Reshape it to match the encoder's output shape\n",
        "\n",
        "    # Define a wrapper function for the encoder, so it can handle just the required output\n",
        "    def partial_encoder(input_data):\n",
        "        return model.encode(input_data)[0]\n",
        "\n",
        "\n",
        "\n",
        "    # Compute Jacobian-vector product\n",
        "    vjp_outputs = torch.autograd.functional.jvp(partial_encoder, g_zi.view(1, 1, image_size, image_size), finite_diff)\n",
        "\n",
        "    # Get the result from the vjp outputs\n",
        "    Jv = vjp_outputs[1].view_as(zi)\n",
        "\n",
        "    # Compute etta_i\n",
        "    etta_i = -Jv\n",
        "\n",
        "    # Free up memory\n",
        "    del g_zi_minus, g_zi, g_zi_plus, finite_diff, Jv, vjp_outputs\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return etta_i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "366305dc",
      "metadata": {
        "id": "366305dc",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def compute_etta_d(model, zi, zi_minus, zi_plus, dt):\n",
        "    # Compute the finite difference\n",
        "    g_zi_minus = model.decode(zi_minus).view(-1)\n",
        "    g_zi = model.decode(zi).view(-1)\n",
        "    g_zi_plus = model.decode(zi_plus).view(-1)\n",
        "\n",
        "    finite_diff = (g_zi_plus - 2 * g_zi + g_zi_minus) / dt\n",
        "    finite_diff = finite_diff.view(1, 1, image_size, image_size)  # Reshape it to match the encoder's output shape\n",
        "\n",
        "    scaled_finite_diff = 0.1 * finite_diff\n",
        "\n",
        "    # Compute vector-Jacobian product\n",
        "    vjp_outputs = torch.autograd.functional.vjp(model.decode, zi, finite_diff.view(1,-1))\n",
        "    # Get the result from the vjp outputs\n",
        "    Jv = vjp_outputs[1].view_as(zi)\n",
        "\n",
        "    # Compute etta_i\n",
        "    etta_i = -Jv #+ scaled_finite_diff)\n",
        "\n",
        "    # Free up memory\n",
        "    del g_zi_minus, g_zi, g_zi_plus, finite_diff, Jv, vjp_outputs\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return etta_i\n",
        "\n",
        "\n",
        "def sum_of_etta_norms(model, z_collection, dt):\n",
        "    norms = []\n",
        "    for j in range(1, len(z_collection) - 1):\n",
        "        etta_j = compute_etta_d(model, z_collection[j], z_collection[j-1], z_collection[j+1], dt)\n",
        "        norms.append(etta_j.norm().pow(2).item())\n",
        "        #del etta_j\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    return sum(norms)\n",
        "\n",
        "\n",
        "def backtracking_line_search(model, z_collection, i, direction, start_alpha, beta, dt, max_iterations, c=0.001):\n",
        "    alpha = start_alpha\n",
        "    current_energy = sum_of_etta_norms(model, z_collection, dt)\n",
        "    gradient_norm_square = direction.norm() ** 2\n",
        "\n",
        "\n",
        "    tmp_z = z_collection[i] - alpha * direction\n",
        "    new_z_collection = [element.clone() for element in z_collection]\n",
        "    new_z_collection[i] = tmp_z\n",
        "    iterations_count = 0\n",
        "\n",
        "    while sum_of_etta_norms(model, new_z_collection, dt) > current_energy - c * alpha * gradient_norm_square:\n",
        "        if iterations_count > max_iterations:\n",
        "            break\n",
        "        alpha *= beta\n",
        "        tmp_z = z_collection[i] - alpha * direction\n",
        "        new_z_collection[i] = tmp_z\n",
        "        iterations_count+=1\n",
        "\n",
        "    return alpha\n",
        "\n",
        "def sum_of_etta_norms_enc(model, z_collection, dt):\n",
        "    norms = []\n",
        "    for j in range(1, len(z_collection) - 2):\n",
        "        etta_j = compute_etta(model, z_collection[j], z_collection[j-1], z_collection[j+1], dt)\n",
        "        norms.append(etta_j.norm().item())\n",
        "        del etta_j\n",
        "        torch.cuda.empty_cache()\n",
        "    return sum(norms)\n",
        "\n",
        "import copy\n",
        "\n",
        "def geodesic_path_algorithm(model, z0, zT, alpha, T, beta, epsilon, max_iterations):\n",
        "    dt = 1.0 / T\n",
        "    z_collection = [z0 + float(i) / T * (zT - z0) for i in range(T)]\n",
        "    #z_collection = [z0 if i == 0 else zT if i == T - 1 else z0 + float(i) / T * (zT - z0) + 0.01 * torch.randn_like(z0) for i in range(T)]\n",
        "\n",
        "    z_list.append([z.clone() for z in z_collection])\n",
        "\n",
        "    iterations = 0\n",
        "\n",
        "    while sum_of_etta_norms(model, z_collection, dt) > epsilon:\n",
        "\n",
        "        if iterations == max_iterations:\n",
        "            break\n",
        "\n",
        "\n",
        "        print(\"Energy\", sum_of_etta_norms(model, z_collection, dt))\n",
        "        etta_norms = []\n",
        "\n",
        "        for i in range(1, T-1):\n",
        "            etta_i = compute_etta_d(model, z_collection[i], z_collection[i-1], z_collection[i+1], dt)\n",
        "            #alpha_i = backtracking_line_search(model, z_collection, i, etta_i, alpha, beta, dt, max_iterations)\n",
        "\n",
        "            alpha_i= alpha\n",
        "            z_collection[i] -= alpha_i * etta_i\n",
        "            etta_norms.append(etta_i.norm().item())\n",
        "\n",
        "            del etta_i\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        #z_collection = [tensor + (torch.randn_like(tensor) * 0.1 if 0 < i < len(z_collection) - 1 else 0)\n",
        "        #                for i, tensor in enumerate(z_collection)]\n",
        "\n",
        "        if (iterations+1) % 10 == 0:\n",
        "            z_list.append([z.clone() for z in z_collection])\n",
        "\n",
        "        iterations+=1\n",
        "\n",
        "    return z_collection\n",
        "\n",
        "image_size = 28\n",
        "import gc\n",
        "latent_dim = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17a29c03",
      "metadata": {
        "id": "17a29c03"
      },
      "outputs": [],
      "source": [
        "all_features = []\n",
        "all_labels = []\n",
        "\n",
        "# Iterate through the DataLoader\n",
        "for features, labels in test_loader:\n",
        "    # Assuming features and labels are tensors, you can append them to the lists\n",
        "    all_features.append(features)\n",
        "    all_labels.append(labels)\n",
        "\n",
        "# Concatenate all features and labels\n",
        "all_features = torch.cat(all_features, dim=0)\n",
        "all_labels = torch.cat(all_labels, dim=0)\n",
        "y_test = all_labels\n",
        "labels = np.unique(all_labels.cpu().numpy())\n",
        "\n",
        "E = vae_net.encoder\n",
        "with torch.no_grad():\n",
        "    EX_test = E(all_features.cuda()).cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "490e60d4",
      "metadata": {
        "id": "490e60d4",
        "outputId": "ac4c6457-7cbe-4a1d-9bf5-be295ca13bb9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=0, learning_rate=70, n_iter=2000, n_iter_without_progress=400, verbose=1)\n",
        "EX2D = tsne.fit_transform(EX_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39fdcff3",
      "metadata": {
        "id": "39fdcff3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62a50006",
      "metadata": {
        "id": "62a50006",
        "outputId": "54c23adf-28c6-41bc-c311-026ffa320d69",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,15))\n",
        "plt.rcParams.update({'font.size': 30})\n",
        "colors = ['brown', 'g', 'olive', 'c', 'm', 'y', 'k', 'lightblue', 'orange', 'gray', \"b\", \"r\"]\n",
        "\n",
        "for i, c in zip(labels, colors):\n",
        "    idx = y_test==i\n",
        "    plt.scatter(EX2D[y_test==i, 0], EX2D[y_test==i,1], c=c, label=str(i))\n",
        "\n",
        "plt.legend(bbox_to_anchor=(-0.1, 0.55), loc=2, borderaxespad=0)\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d70a862",
      "metadata": {
        "id": "2d70a862",
        "outputId": "64b47117-c0af-4d75-bff5-2f324d3e8f07"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (5,10))\n",
        "out = vutils.make_grid(train_images[0:20], normalize=True)\n",
        "plt.imshow(out.numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb744851",
      "metadata": {
        "id": "cb744851"
      },
      "outputs": [],
      "source": [
        "test_z,_,_ = vae_net.encode(train_images[49].view(1,1,28,28).cuda())\n",
        "x,_,_ =vae_net.encode(train_images[16].view(1,1,28,28).cuda())\n",
        "zero_z = torch.FloatTensor(1,latent_dim).zero_().cuda()\n",
        "white_z,_,_ = vae_net.encode(torch.ones((1, 1, 28, 28)).cuda())\n",
        "black_z,_,_ = vae_net.encode(torch.zeros((1, 1, 28, 28)).cuda())\n",
        "\n",
        "z_list = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d819928",
      "metadata": {
        "id": "8d819928",
        "outputId": "c28b786e-b817-49be-9712-7c1bbda5bb4a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "path = geodesic_path_algorithm(vae_net, black_z, x, alpha=0.01, T=10, beta=0.7, epsilon=10, max_iterations=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c95dcf33",
      "metadata": {
        "id": "c95dcf33",
        "outputId": "b3847af3-80a8-4522-bf6c-41e007c8cae5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "interpolated_geodesic_images = [vae_net.decode(vec).view(-1,28,28) for vec in path]\n",
        "\n",
        "vv = [v for v in interpolated_geodesic_images]\n",
        "plt.figure(figsize = (20,5))\n",
        "out = vutils.make_grid(vv, normalize=True)\n",
        "plt.imshow(out.cpu().numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37f08f34",
      "metadata": {
        "id": "37f08f34",
        "scrolled": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34916109",
      "metadata": {
        "id": "34916109",
        "outputId": "85084124-7db4-46b5-913f-a2c12f6be14d"
      },
      "outputs": [],
      "source": [
        "\n",
        "def interpolate(start, end, steps):\n",
        "    \"\"\"Generate interpolated vectors between start and end.\"\"\"\n",
        "    interpolation = [start + float(i) / steps * (end - start) for i in range(steps)]\n",
        "    #interpolation = [start if i == 0 else end if i == steps - 1 else start + float(i) / steps * (end - start) + 0.3 * torch.randn_like(start) for i in range(steps)]\n",
        "    return interpolation\n",
        "\n",
        "# Interpolate between zero_z and x\n",
        "interpolated_vectors = interpolate(black_z, x, 10)\n",
        "\n",
        "# Decode these vectors to images\n",
        "interpolated_rec_images = [vae_net.decode(vec).view(-1,28,28) for vec in interpolated_vectors]\n",
        "interpolated_images = interpolate(interpolated_rec_images[0], interpolated_rec_images[-1], 10)\n",
        "\n",
        "vv = [v for v in interpolated_rec_images]\n",
        "plt.figure(figsize = (20,5))\n",
        "out = vutils.make_grid(vv, normalize=True)\n",
        "plt.imshow(out.cpu().numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d8210ed",
      "metadata": {
        "id": "9d8210ed",
        "outputId": "b41aa8c3-efc7-46a8-8920-3c10fa6fbe26"
      },
      "outputs": [],
      "source": [
        "vv = [v for v in interpolated_images]\n",
        "plt.figure(figsize = (20,5))\n",
        "out = vutils.make_grid(interpolated_images, normalize=True)\n",
        "plt.imshow(out.cpu().numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae54d24e",
      "metadata": {
        "id": "ae54d24e"
      },
      "outputs": [],
      "source": [
        "class VAEClassifier(nn.Module):\n",
        "    def __init__(self, pretrained_vae, num_classes=10):\n",
        "        super(VAEClassifier, self).__init__()\n",
        "\n",
        "        # Use the encoder from the pretrained VAE\n",
        "        self.encoder = pretrained_vae.encoder\n",
        "        self.mu = pretrained_vae.mu\n",
        "\n",
        "        # Freeze the encoder parameters\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        for param in self.mu.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Define a classifier - this is just an example, you can add more layers if needed\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(32, 16),  # 512 is the output size of the encoder\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(16, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.mu(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db82a20b",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            ""
          ]
        },
        "id": "db82a20b",
        "outputId": "b7c878a7-8b2f-4bda-91e0-5f1e89189fdc",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "classifier = VAEClassifier(pretrained_vae=vae_net).to(device)\n",
        "\n",
        "num_epochs = 20\n",
        "optimizer = optim.Adam(classifier.classifier.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "def test_model():\n",
        "    classifier.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            output = classifier(data)\n",
        "            test_loss += criterion(output, target).item()  # Sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    return test_loss\n",
        "\n",
        "# Existing training loop starts here...\n",
        "for epoch in range(num_epochs):\n",
        "    classifier.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(tqdm(train_loader, leave=False)):\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = classifier(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Print epoch statistics\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    print('Epoch: {} Average training loss: {:.4f}'.format(epoch, train_loss))\n",
        "\n",
        "    # Test the model at the end of each epoch\n",
        "    test_model()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90138deb",
      "metadata": {
        "id": "90138deb"
      },
      "outputs": [],
      "source": [
        "# Boilerplate imports.\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "from matplotlib import pylab as P\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "\n",
        "# From our repository.\n",
        "\n",
        "%matplotlib inline\n",
        "def ShowImage(im, title='', ax=None):\n",
        "    if ax is None:\n",
        "        P.figure()\n",
        "    P.axis('off')\n",
        "    P.imshow(im)\n",
        "    P.title(title)\n",
        "\n",
        "def ShowGrayscaleImage(im, title='', ax=None):\n",
        "    if ax is None:\n",
        "        P.figure()\n",
        "    im_min = im.min()\n",
        "    im_max = im.max()\n",
        "    im_normalized = (im - im_min) / (im_max - im_min)\n",
        "\n",
        "    P.axis('off')\n",
        "    P.imshow(im_normalized.squeeze(), cmap=P.cm.gray, vmin=0, vmax=1)\n",
        "    P.title(title)\n",
        "\n",
        "def ShowHeatMap(im, title, ax=None):\n",
        "    if ax is None:\n",
        "        P.figure()\n",
        "    P.axis('off')\n",
        "    P.imshow(im, cmap='inferno')\n",
        "    P.title(title)\n",
        "\n",
        "def LoadImage(file_path):\n",
        "    im = PIL.Image.open(file_path)\n",
        "    im = im.resize((299, 299))\n",
        "    im = np.asarray(im)\n",
        "    return im\n",
        "\n",
        "transformer = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "def PreprocessImages(images):\n",
        "    # assumes input is 4-D, with range [0,255]\n",
        "    #\n",
        "    # torchvision have color channel as first dimension\n",
        "    # with normalization relative to mean/std of ImageNet:\n",
        "    #    https://pytorch.org/vision/stable/models.html\n",
        "    images = np.array(images)\n",
        "    images = images/255\n",
        "    images = np.transpose(images, (0,3,1,2))\n",
        "    images = torch.tensor(images, dtype=torch.float32)\n",
        "    images = transformer.forward(images)\n",
        "    return images.requires_grad_(True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a933e47",
      "metadata": {
        "id": "3a933e47"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "INPUT_OUTPUT_GRADIENTS = 'INPUT_OUTPUT_GRADIENTS'\n",
        "\n",
        "SHAPE_ERROR_MESSAGE = {\n",
        "    INPUT_OUTPUT_GRADIENTS: (\n",
        "        'Expected key INPUT_OUTPUT_GRADIENTS to be the same shape as input '\n",
        "        'x_value_batch - expected {}, actual {}'\n",
        "    ),\n",
        "}\n",
        "\n",
        "class IntegratedGradients:\n",
        "    \"\"\"Class that implements the integrated gradients method.\n",
        "\n",
        "    https://arxiv.org/abs/1703.01365\n",
        "    \"\"\"\n",
        "\n",
        "    expected_keys = [INPUT_OUTPUT_GRADIENTS]\n",
        "\n",
        "    def GetMask(self, x_value, call_model_function, variant='vanilla',\n",
        "                call_model_args=None, x_baseline=None, x_steps=25,\n",
        "                batch_size=1, interpolation_points=None):\n",
        "        \"\"\"Returns an integrated gradients mask.\n",
        "\n",
        "        Args:\n",
        "          ... [Same as previously described]\n",
        "          variant: Either 'vanilla' or 'custom'. If 'custom', interpolation_points should be provided.\n",
        "          interpolation_points: Points to be used for the 'custom' variant of integrated gradients.\n",
        "                                List of ndarrays.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        if variant == 'vanilla':\n",
        "            if x_baseline is None:\n",
        "                x_baseline = torch.zeros_like(x_value)\n",
        "            assert x_baseline.shape == x_value.shape\n",
        "\n",
        "            x_diff = x_value - x_baseline\n",
        "\n",
        "            total_gradients = torch.zeros_like(x_value, dtype=torch.float32)\n",
        "\n",
        "            x_step_batched = []\n",
        "            for alpha in np.linspace(0, 1, x_steps):\n",
        "                x_step = x_baseline + alpha * x_diff\n",
        "                x_step_batched.append(x_step)\n",
        "                if len(x_step_batched) == batch_size or alpha == 1:\n",
        "                    x_step_batched = torch.stack(x_step_batched)\n",
        "                    call_model_output = call_model_function(\n",
        "                        x_step_batched,\n",
        "                        call_model_args=call_model_args,\n",
        "                        expected_keys=self.expected_keys)\n",
        "\n",
        "                    print(\"shape of x batched\", x_step_batched.shape)\n",
        "\n",
        "                    self.format_and_check_call_model_output(call_model_output,\n",
        "                                                            x_step_batched.shape,\n",
        "                                                            self.expected_keys)\n",
        "\n",
        "                    total_gradients += call_model_output[INPUT_OUTPUT_GRADIENTS].sum(axis=0)\n",
        "                    x_step_batched = []\n",
        "\n",
        "            return total_gradients * x_diff / x_steps\n",
        "\n",
        "        elif variant == 'manifold':\n",
        "            assert interpolation_points is not None #Provide interpolation points for custom variant.\n",
        "            x_diff = interpolation_points[-1] - interpolation_points[0]\n",
        "\n",
        "            total_gradients = torch.zeros_like(x_diff, dtype=torch.float32)\n",
        "            x_step_batched = torch.stack(interpolation_points)\n",
        "            #for x_step in interpolation_points:\n",
        "            call_model_output = call_model_function(\n",
        "                x_step_batched,\n",
        "                call_model_args=call_model_args,\n",
        "                expected_keys=self.expected_keys)\n",
        "\n",
        "            self.format_and_check_call_model_output(call_model_output,\n",
        "                                                    x_step_batched.shape,\n",
        "                                                    self.expected_keys)\n",
        "            total_gradients += call_model_output[INPUT_OUTPUT_GRADIENTS].sum(axis=0)\n",
        "\n",
        "            return total_gradients * x_diff / len(interpolation_points)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Invalid variant provided.\")\n",
        "\n",
        "    def format_and_check_call_model_output(self, output, input_shape, expected_keys):\n",
        "        \"\"\"Converts keys in the output into an np.ndarray, and confirms its shape.\n",
        "\n",
        "        Args:\n",
        "          ... [Same as previously described]\n",
        "        \"\"\"\n",
        "        check_full_shape = [INPUT_OUTPUT_GRADIENTS]\n",
        "        for expected_key in expected_keys:\n",
        "            output[expected_key] = np.asarray(output[expected_key])\n",
        "            expected_shape = input_shape\n",
        "            actual_shape = output[expected_key].shape\n",
        "            if expected_key not in check_full_shape:\n",
        "                expected_shape = expected_shape[0]\n",
        "\n",
        "                actual_shape = actual_shape[0]\n",
        "            if expected_shape != actual_shape:\n",
        "                raise ValueError(SHAPE_ERROR_MESSAGE[expected_key].format(\n",
        "                                expected_shape, actual_shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4457d476",
      "metadata": {
        "id": "4457d476"
      },
      "outputs": [],
      "source": [
        "class_idx_str = 'class_idx_str'\n",
        "classifier.eval()\n",
        "def call_model_function(images, call_model_args=None, expected_keys=None):\n",
        "    tensor_images = images.cuda()\n",
        "    tensor_images.requires_grad_(True)\n",
        "\n",
        "    target_class_idx =  call_model_args[class_idx_str]\n",
        "    output = classifier(tensor_images)\n",
        "    #m = torch.nn.Softmax(dim=1)\n",
        "    #output = m(output)\n",
        "    if INPUT_OUTPUT_GRADIENTS in expected_keys:\n",
        "        outputs = output[:,target_class_idx]\n",
        "        grads = torch.autograd.grad(outputs, tensor_images, grad_outputs=torch.ones_like(outputs))\n",
        "        grads = torch.movedim(grads[0], 2, 3)\n",
        "        gradients = grads.cpu().detach().numpy()\n",
        "        return {INPUT_OUTPUT_GRADIENTS: gradients}\n",
        "    else:\n",
        "        one_hot = torch.zeros_like(output)\n",
        "        one_hot[:,target_class_idx] = 1\n",
        "        model.zero_grad()\n",
        "        output.backward(gradient=one_hot, retain_graph=True)\n",
        "        return conv_layer_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0b47c87",
      "metadata": {
        "id": "d0b47c87",
        "outputId": "b235ecc8-8596-4e7c-bbf2-5d4f1bd409b0"
      },
      "outputs": [],
      "source": [
        "#im_orig = LoadImage('./doberman.png')\n",
        "#im_tensor = PreprocessImages([im_orig])\n",
        "# Show the image\n",
        "#ShowImage(im_orig)\n",
        "class_idx_str = 'class_idx_str'\n",
        "\n",
        "predictions = classifier(train_images[8].cuda())\n",
        "m = torch.nn.Softmax(dim=1)\n",
        "predictions = m(predictions)\n",
        "predictions = predictions.cpu().detach().numpy()\n",
        "prediction_class = np.argmax(predictions[0])\n",
        "call_model_args = {class_idx_str: prediction_class}\n",
        "\n",
        "print(\"Prediction class: \" + str(prediction_class))  # Should be a doberman, class idx = 236\n",
        "#im = im_orig.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2790b1c3",
      "metadata": {
        "id": "2790b1c3",
        "outputId": "dd70a0ac-fc1f-4ff1-fb10-2fc8210a0474"
      },
      "outputs": [],
      "source": [
        "def ShowGrayscaleImage(im, title='', ax=None, vmin=0, vmax=1):\n",
        "    if ax is None:\n",
        "        P.figure()\n",
        "    im_min = im.min()\n",
        "    im_max = im.max()\n",
        "    im_normalized = im #(im - im_min) / (im_max - im_min)\n",
        "    P.axis('off')\n",
        "    P.imshow(im_normalized.squeeze(), cmap=P.cm.gray, vmin=vmin, vmax=vmax)\n",
        "    P.title(title)\n",
        "\n",
        "ROWS = 1\n",
        "COLS = 5\n",
        "UPSCALE_FACTOR = 10\n",
        "P.figure(figsize=(ROWS * UPSCALE_FACTOR, COLS * UPSCALE_FACTOR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "591cb966",
      "metadata": {
        "id": "591cb966",
        "outputId": "13fe7658-5511-4cbf-e12a-d33535d655a0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "integrated_gradients = IntegratedGradients()\n",
        "\n",
        "# Baseline is a black image.\n",
        "baseline = torch.zeros(test_images[1].shape)\n",
        "baseline = interpolated_geodesic_images[0].cpu()\n",
        "#baseline = torch.ones(test_images[3].shape)\n",
        "\n",
        "test = interpolated_geodesic_images[-1].cuda()\n",
        "# Compute the vanilla mask and the smoothed mask.\n",
        "vanilla_ig = integrated_gradients.GetMask(x_value=test.cpu(),\n",
        "                                                  call_model_function=call_model_function,\n",
        "                                                  variant='vanilla', call_model_args=call_model_args,\n",
        "                                                  x_steps=20, x_baseline=baseline, batch_size=100)\n",
        "\n",
        "\n",
        "geodesic_points = [point.cpu() for point in interpolated_geodesic_images]\n",
        "\n",
        "linear_points = [point.cpu() for point in interpolated_rec_images]\n",
        "\n",
        "ig_geodesic = integrated_gradients.GetMask(x_value=test.cpu(),\n",
        "                                                  call_model_function=call_model_function,\n",
        "                                                  variant='manifold', call_model_args=call_model_args,\n",
        "                                                  x_steps=20, x_baseline=baseline, batch_size=100,\n",
        "                                                 interpolation_points=geodesic_points)\n",
        "\n",
        "ig_linear = integrated_gradients.GetMask(x_value=test.cpu(),\n",
        "                                                  call_model_function=call_model_function,\n",
        "                                                  variant='manifold', call_model_args=call_model_args,\n",
        "                                                  x_steps=20, x_baseline=baseline, batch_size=100,\n",
        "                                                 interpolation_points=linear_points)\n",
        "\n",
        "# Render the saliency masks.\n",
        "ShowGrayscaleImage(vanilla_ig.cpu().detach().numpy(), title='Vanilla IG', ax=P.subplot(ROWS, COLS, 1), vmin=-0.00001, vmax=0.005)\n",
        "ShowGrayscaleImage(ig_geodesic.cpu().detach().numpy(), title='IG Geodesic', ax=P.subplot(ROWS, COLS, 2), vmin=-0.00001, vmax=0.005)\n",
        "ShowGrayscaleImage(ig_linear.cpu().detach().numpy(), title='IG Linear', ax=P.subplot(ROWS, COLS, 3), vmin=-0.00001, vmax=0.005)\n",
        "#ShowGrayscaleImage(attributions_ig.cpu().detach().numpy(), title='Captum IG', ax=P.subplot(ROWS, COLS, 4), vmin=-0.00001, vmax=0.005)\n",
        "\n",
        "ShowGrayscaleImage(test.cpu().detach().numpy(), title='Original', ax=P.subplot(ROWS, COLS, 4), vmin=0, vmax=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c332efd",
      "metadata": {
        "id": "8c332efd",
        "outputId": "4582395c-c4bd-43ee-af29-032de2e8e56b"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (5,10))\n",
        "out = vutils.make_grid(train_images[0:16], normalize=True)\n",
        "plt.imshow(out.numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c30579bb",
      "metadata": {
        "id": "c30579bb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d33fab5",
      "metadata": {
        "id": "9d33fab5",
        "outputId": "82381903-6a31-4576-ff28-5f3e8e53ff8a"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(linear_det_val, label='Linear Path', marker='o', linestyle='-')\n",
        "plt.plot(geodesic_det_val, label='Geodesic Path', marker='x', linestyle='--')\n",
        "plt.yscale('log')  # Use a logarithmic scale for clarity due to the large range\n",
        "plt.title('Determinant of G along Linear and Geodesic Paths')\n",
        "plt.xlabel('Point Index')\n",
        "\n",
        "plt.ylabel('Determinant Value (Log Scale)')\n",
        "plt.legend()\n",
        "plt.grid(True, which=\"both\", ls=\"--\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89891915",
      "metadata": {
        "id": "89891915",
        "outputId": "7fea8da5-2dcb-4c88-db97-d8a877979607",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,6))\n",
        "cm = plt.get_cmap(\"viridis\")  # you can change 'viridis' to another colormap name if desired\n",
        "num_of_paths = len(path_list)\n",
        "colors = [cm(0.2*i/num_of_paths) for i in range(num_of_paths)]\n",
        "\n",
        "for index, (sub_list, color) in enumerate(zip(path_list, colors)):\n",
        "    sub_list = [val.cpu().numpy() for val in sub_list]\n",
        "    plt.plot(sub_list, color=color, label=f\"pth_iteration{index*10-index}\")\n",
        "\n",
        "\n",
        "plt.yscale('log')  # Use a logarithmic scale for clarity due to the large range\n",
        "plt.title('Determinant of G along every updated path between linear and geodesic')\n",
        "plt.xlabel('Point Index')\n",
        "\n",
        "plt.ylabel('Determinant Value (Log Scale)')\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "plt.grid(True, which=\"both\", ls=\"--\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0172c93d",
      "metadata": {
        "id": "0172c93d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
