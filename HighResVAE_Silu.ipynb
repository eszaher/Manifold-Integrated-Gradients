{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13c9b9b6",
      "metadata": {
        "id": "13c9b9b6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.datasets as Datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.utils as vutils\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from torchvision.transforms import functional as TF\n",
        "import PIL\n",
        "\n",
        "latent_channels = 64\n",
        "batch_size = 64\n",
        "image_size = 192\n",
        "lr = 5e-4\n",
        "nepoch = 100\n",
        "start_epoch = 0\n",
        "dataset_root = \"\"\n",
        "model_name = \"oxford_pets_VAE_Perceptual_03_white_black_silu\"\n",
        "torch.manual_seed(7)  # For reproducibility\n",
        "\n",
        "save_dir = os.getcwd()\n",
        "load_checkpoint  = True\n",
        "im_size = 192\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "gpu_indx  = 0\n",
        "device = torch.device(gpu_indx if use_cuda else \"cpu\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0086738f",
      "metadata": {
        "id": "0086738f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, data_dir, file_names, transform=None, num_black_images=350, num_white_images=350, image_size=(3, 192, 192)):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.file_names = file_names\n",
        "        self.image_size = image_size\n",
        "\n",
        "        # Add placeholders for black images\n",
        "        self.black_image_placeholder = \"<black_image>\"\n",
        "        self.file_names.extend([self.black_image_placeholder] * num_black_images)\n",
        "        self.white_image_placeholder = \"<white_image>\"\n",
        "        self.file_names.extend([self.white_image_placeholder] * num_white_images)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.file_names[idx] == self.black_image_placeholder:\n",
        "            # Create a black image\n",
        "            black_image = torch.zeros(self.image_size)\n",
        "            black_image = TF.to_pil_image(black_image)\n",
        "            return transforms.ToTensor()(black_image) #self.transform(black_image) if self.transform else black_image\n",
        "\n",
        "        if self.file_names[idx] == self.white_image_placeholder:\n",
        "            # Create a black image\n",
        "            white_image = torch.ones(self.image_size)\n",
        "            white_image = TF.to_pil_image(white_image)\n",
        "            return transforms.ToTensor()(white_image) #self.transform(white_image) if self.transform else black_image\n",
        "\n",
        "\n",
        "        img_name = os.path.join(self.data_dir, self.file_names[idx])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(im_size, im_size)),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.Resize(size=(im_size, im_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3424399",
      "metadata": {
        "id": "c3424399"
      },
      "outputs": [],
      "source": [
        "#!pip install scikit-learn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "all_images = [img for img in os.listdir(dataset_root) if img.endswith('.jpg')]\n",
        "train_images, test_images = train_test_split(all_images, test_size=0.05, random_state=7)\n",
        "\n",
        "train_dataset = CustomImageDataset(data_dir=dataset_root, file_names=train_images, transform=train_transform, num_black_images=500)\n",
        "test_dataset = CustomImageDataset(data_dir=dataset_root, file_names=test_images, transform=test_transform, num_black_images=500)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "541cd829",
      "metadata": {
        "id": "541cd829"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1a3486d",
      "metadata": {
        "id": "e1a3486d",
        "outputId": "9e96808e-a93d-4c62-88b7-bdc5003b1f08"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(train_loader)\n",
        "train_images = next(dataiter)\n",
        "train_images.shape\n",
        "\n",
        "test_dataiter = iter(test_loader)\n",
        "test_images = next(test_dataiter)\n",
        "test_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0443b9e2",
      "metadata": {
        "id": "0443b9e2",
        "outputId": "5fe40753-5826-480e-dda7-b752633d8f3f"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20,10))\n",
        "out = vutils.make_grid(test_images[0:64], normalize=True)\n",
        "plt.imshow(out.numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b1ca477",
      "metadata": {
        "id": "3b1ca477"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "class VGG19(nn.Module):\n",
        "    \"\"\"\n",
        "     Simplified version of the VGG19 \"feature\" block\n",
        "     This module's only job is to return the \"feature loss\" for the inputs\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channel_in=3, width=64):\n",
        "        super(VGG19, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(channel_in, width, 3, 1, 1)\n",
        "        self.conv2 = nn.Conv2d(width, width, 3, 1, 1)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(width, 2 * width, 3, 1, 1)\n",
        "        self.conv4 = nn.Conv2d(2 * width, 2 * width, 3, 1, 1)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(2 * width, 4 * width, 3, 1, 1)\n",
        "        self.conv6 = nn.Conv2d(4 * width, 4 * width, 3, 1, 1)\n",
        "        self.conv7 = nn.Conv2d(4 * width, 4 * width, 3, 1, 1)\n",
        "        self.conv8 = nn.Conv2d(4 * width, 4 * width, 3, 1, 1)\n",
        "\n",
        "        self.conv9 = nn.Conv2d(4 * width, 8 * width, 3, 1, 1)\n",
        "        self.conv10 = nn.Conv2d(8 * width, 8 * width, 3, 1, 1)\n",
        "        self.conv11 = nn.Conv2d(8 * width, 8 * width, 3, 1, 1)\n",
        "        self.conv12 = nn.Conv2d(8 * width, 8 * width, 3, 1, 1)\n",
        "\n",
        "        self.conv13 = nn.Conv2d(8 * width, 8 * width, 3, 1, 1)\n",
        "        self.conv14 = nn.Conv2d(8 * width, 8 * width, 3, 1, 1)\n",
        "        self.conv15 = nn.Conv2d(8 * width, 8 * width, 3, 1, 1)\n",
        "        self.conv16 = nn.Conv2d(8 * width, 8 * width, 3, 1, 1)\n",
        "\n",
        "        self.mp = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.load_params_()\n",
        "\n",
        "    def load_params_(self):\n",
        "        # Download and load Pytorch's pre-trained weights\n",
        "        state_dict = torch.hub.load_state_dict_from_url('https://download.pytorch.org/models/vgg19-dcbb9e9d.pth')\n",
        "        for ((name, source_param), target_param) in zip(state_dict.items(), self.parameters()):\n",
        "            target_param.data = source_param.data\n",
        "            target_param.requires_grad = False\n",
        "\n",
        "    def feature_loss(self, x):\n",
        "        return (x[:x.shape[0] // 2] - x[x.shape[0] // 2:]).pow(2).mean()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Expects x to be the target and source to concatenated on dimension 0\n",
        "        :return: Feature loss\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        loss = self.feature_loss(x)\n",
        "        x = self.conv2(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.mp(self.relu(x))  # 64x64\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv4(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.mp(self.relu(x))  # 32x32\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv6(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv7(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv8(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.mp(self.relu(x))  # 16x16\n",
        "\n",
        "        x = self.conv9(x)\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv10(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv11(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv12(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.mp(self.relu(x))  # 8x8\n",
        "\n",
        "        x = self.conv13(x)\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv14(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv15(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "        x = self.conv16(self.relu(x))\n",
        "        loss += self.feature_loss(x)\n",
        "\n",
        "        return loss/16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65298f2a",
      "metadata": {
        "id": "65298f2a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "\n",
        "\n",
        "class ResDown(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual down sampling block for the encoder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channel_in, channel_out, kernel_size=3):\n",
        "        super(ResDown, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channel_in, channel_out // 2, kernel_size, 2, kernel_size // 2)\n",
        "        self.bn1 = nn.BatchNorm2d(channel_out // 2, eps=1e-4)\n",
        "        self.conv2 = nn.Conv2d(channel_out // 2, channel_out, kernel_size, 1, kernel_size // 2)\n",
        "        self.bn2 = nn.BatchNorm2d(channel_out, eps=1e-4)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 2, kernel_size // 2)\n",
        "\n",
        "        self.act_fnc = nn.SiLU() # Change activation here\n",
        "\n",
        "    def forward(self, x):\n",
        "        #skip = self.conv3(x) $ Ignore skip connections\n",
        "        x = self.act_fnc(self.bn1(self.conv1(x)))\n",
        "        x = self.conv2(x)\n",
        "        return self.act_fnc(self.bn2(x))#+ skip))\n",
        "\n",
        "\n",
        "class ResUp(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual up sampling block for the decoder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channel_in, channel_out, kernel_size=3, scale_factor=2):\n",
        "        super(ResUp, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(channel_in, channel_in // 2, kernel_size, 1, kernel_size // 2)\n",
        "        self.bn1 = nn.BatchNorm2d(channel_in // 2, eps=1e-4)\n",
        "        self.conv2 = nn.Conv2d(channel_in // 2, channel_out, kernel_size, 1, kernel_size // 2)\n",
        "        self.bn2 = nn.BatchNorm2d(channel_out, eps=1e-4)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 1, kernel_size // 2)\n",
        "\n",
        "        self.up_nn = nn.Upsample(scale_factor=scale_factor, mode=\"nearest\")\n",
        "\n",
        "        self.act_fnc = nn.ELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up_nn(x)\n",
        "        #skip = self.conv3(x)\n",
        "        x = self.act_fnc(self.bn1(self.conv1(x)))\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        return self.act_fnc(self.bn2(x)) #+ skip))\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channel_in, channel_out, kernel_size=3, act=nn.ELU()):\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(channel_in, channel_in // 2, kernel_size, 1, kernel_size // 2)\n",
        "        self.bn1 = nn.BatchNorm2d(channel_in // 2, eps=1e-4)\n",
        "        self.conv2 = nn.Conv2d(channel_in // 2, channel_out, kernel_size, 1, kernel_size // 2)\n",
        "        self.bn2 = nn.BatchNorm2d(channel_out, eps=1e-4)\n",
        "\n",
        "        if not channel_in == channel_out:\n",
        "            self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 1, kernel_size // 2)\n",
        "        else:\n",
        "            self.conv3 = nn.Identity()\n",
        "\n",
        "        self.act_fnc = act\n",
        "\n",
        "    def forward(self, x):\n",
        "        #skip = self.conv3(x)\n",
        "        x = self.act_fnc(self.bn1(self.conv1(x)))\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        return self.act_fnc(self.bn2(x))#+ skip))\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, ch=64, blocks=(1, 2, 4, 8), latent_channels=512):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv_in = nn.Conv2d(channels, blocks[0] * ch, 3, 1, 1)\n",
        "\n",
        "        widths_in = list(blocks)\n",
        "        widths_out = list(blocks[1:]) + [blocks[-1]]\n",
        "\n",
        "        layer_blocks = []\n",
        "\n",
        "        for w_in, w_out in zip(widths_in, widths_out):\n",
        "            layer_blocks.append(ResDown(w_in * ch, w_out * ch))\n",
        "\n",
        "        layer_blocks.append(ResBlock(blocks[-1] * ch, blocks[-1] * ch, kernel_size=3, act=nn.SiLU()))\n",
        "        layer_blocks.append(ResBlock(blocks[-1] * ch, blocks[-1] * ch, kernel_size=3, act=nn.SiLU()))\n",
        "\n",
        "        self.res_blocks = nn.Sequential(*layer_blocks)\n",
        "\n",
        "        self.conv_mu = nn.Conv2d(blocks[-1] * ch, latent_channels, 1, 1)\n",
        "        self.conv_log_var = nn.Conv2d(blocks[-1] * ch, latent_channels, 1, 1)\n",
        "        self.act_fnc = nn.SiLU() # Change activation here\n",
        "\n",
        "    def sample(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x, sample=False):\n",
        "        x = self.act_fnc(self.conv_in(x))\n",
        "        x = self.res_blocks(x)\n",
        "\n",
        "        mu = self.conv_mu(x)\n",
        "        log_var = self.conv_log_var(x)\n",
        "\n",
        "        if self.training or sample:\n",
        "            x = self.sample(mu, log_var)\n",
        "        else:\n",
        "            x = mu\n",
        "\n",
        "        return x, mu, log_var\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder block\n",
        "    Built to be a mirror of the encoder block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, ch=64, blocks=(1, 2, 4, 8), latent_channels=512):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv_in = nn.Conv2d(latent_channels, ch * blocks[-1], 1, 1)\n",
        "\n",
        "        widths_out = list(blocks)[::-1]\n",
        "        widths_in = (list(blocks[1:]) + [blocks[-1]])[::-1]\n",
        "\n",
        "        layer_blocks = [ResBlock(blocks[-1] * ch, blocks[-1] * ch, kernel_size=3, act=nn.ELU()),\n",
        "                        ResBlock(blocks[-1] * ch, blocks[-1] * ch, kernel_size=3, act=nn.ELU())]\n",
        "\n",
        "        for w_in, w_out in zip(widths_in, widths_out):\n",
        "            layer_blocks.append(ResUp(w_in * ch, w_out * ch))\n",
        "\n",
        "        self.res_blocks = nn.Sequential(*layer_blocks)\n",
        "\n",
        "        self.conv_out = nn.Conv2d(blocks[0] * ch, channels, 3, 1, 1)\n",
        "        self.act_fnc = nn.ELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act_fnc(self.conv_in(x))\n",
        "        x = self.res_blocks(x)\n",
        "        mu = torch.tanh(self.conv_out(x))\n",
        "        return mu\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    \"\"\"\n",
        "    VAE network, uses the above encoder and decoder blocks\n",
        "    \"\"\"\n",
        "    def __init__(self, channel_in=3, ch=64, blocks=(1, 2, 4, 8), latent_channels=512):\n",
        "        super(VAE, self).__init__()\n",
        "        \"\"\"Res VAE Network\n",
        "        channel_in  = number of channels of the image\n",
        "        z = the number of channels of the latent representation\n",
        "        (for a 64x64 image this is the size of the latent vector)\n",
        "        \"\"\"\n",
        "\n",
        "        self.encoder = Encoder(channel_in, ch=ch, blocks=blocks, latent_channels=latent_channels)\n",
        "        self.decoder = Decoder(channel_in, ch=ch, blocks=blocks, latent_channels=latent_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoding, mu, log_var = self.encoder(x)\n",
        "        recon_img = self.decoder(encoding)\n",
        "        return recon_img, mu, log_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2749d5df",
      "metadata": {
        "id": "2749d5df"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def kl_loss(mu, logvar):\n",
        "    return -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36711e1d",
      "metadata": {
        "id": "36711e1d"
      },
      "outputs": [],
      "source": [
        "feature_extractor = VGG19().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfb37178",
      "metadata": {
        "id": "bfb37178",
        "outputId": "8653d11b-86f8-40c0-de63-fc650852c60c"
      },
      "outputs": [],
      "source": [
        "vae_net = VAE(channel_in=3, ch=64, blocks=(1, 2, 4, 8), latent_channels=64).to(device)\n",
        "# setup optimizer\n",
        "optimizer = optim.Adam(vae_net.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "#Loss function\n",
        "loss_log = []\n",
        "from torchsummary import summary\n",
        "summary(vae_net, (3,im_size,im_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7bfb867",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "5b7d674a64764d81901993c41eeb1f6e",
            "",
            "39b2755ba8014e6da6ebfaa0bc6c5b90"
          ]
        },
        "id": "c7bfb867",
        "outputId": "874fe67e-0b54-44e0-81a3-c295f147eff9"
      },
      "outputs": [],
      "source": [
        "nepoch = 150\n",
        "\n",
        "for epoch in trange(start_epoch, nepoch, leave=False):\n",
        "    train_loss = 0\n",
        "    train_recon_loss = 0\n",
        "    train_kld_loss = 0\n",
        "    train_perceptual_loss = 0\n",
        "\n",
        "    vae_net.train()\n",
        "    for i, images in enumerate(tqdm(train_loader, leave=False)):\n",
        "        images = images.to(device)\n",
        "\n",
        "        recon_img, mu, logvar = vae_net(images)\n",
        "        #VAE loss\n",
        "        kl_loss_ = kl_loss(mu, logvar)\n",
        "        mse_loss = F.mse_loss(recon_img, images)\n",
        "\n",
        "        #Perception loss\n",
        "        feat_in = torch.cat((recon_img, images), 0)\n",
        "        feature_loss = feature_extractor(feat_in)\n",
        "\n",
        "        loss = kl_loss_ + mse_loss + feature_loss\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_recon_loss += mse_loss.item()\n",
        "        train_kld_loss += kl_loss_.item()\n",
        "        train_perceptual_loss += feature_loss.item()\n",
        "\n",
        "        loss_log.append(loss.item())\n",
        "        vae_net.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = train_loss / len(train_loader)  # Calculate average loss for the epoch\n",
        "    avg_recon_loss = train_recon_loss / len(train_loader)\n",
        "    avg_kld_loss = train_kld_loss / len(train_loader)\n",
        "    avg_perceptual_loss = train_perceptual_loss / len(train_loader)\n",
        "\n",
        "    #In eval mode the model will use mu as the encoding instead of sampling from the distribution\n",
        "    vae_net.eval()\n",
        "    with torch.no_grad():\n",
        "        recon_img, _, _ = vae_net(test_images.to(device))\n",
        "        img_cat = torch.cat((recon_img.cpu(), test_images), 2)\n",
        "\n",
        "        vutils.save_image(img_cat,\n",
        "                          \"%s/%s/%s_%d.png\" % (save_dir, \"Results\" , model_name, image_size),\n",
        "                          normalize=True)\n",
        "\n",
        "        #Save a checkpoint\n",
        "        torch.save({\n",
        "                    'epoch'                         : epoch,\n",
        "                    'loss_log'                      : loss_log,\n",
        "                    'model_state_dict'              : vae_net.state_dict(),\n",
        "                    'optimizer_state_dict'          : optimizer.state_dict()\n",
        "\n",
        "                     }, save_dir + \"/Models/\" + model_name + \"_\" + str(image_size) + \".pt\")\n",
        "    print(f'Epoch {epoch}/{nepoch} - Avg Total Loss: {avg_loss} - Avg Recon Loss: {avg_recon_loss}\\\n",
        "    - Avg KLD Loss: {avg_kld_loss} - Avg Percp Loss: {avg_perceptual_loss}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e0de1da",
      "metadata": {
        "id": "1e0de1da",
        "outputId": "cce7efed-fa00-4933-e982-fa77faf4b443"
      },
      "outputs": [],
      "source": [
        "# Run this to load the saved model: load_checkpoint should be True\n",
        "\n",
        "#Create the save directory if it does note exist\n",
        "if not os.path.isdir(save_dir + \"/Models\"):\n",
        "    os.makedirs(save_dir + \"/Models\")\n",
        "if not os.path.isdir(save_dir + \"/Results\"):\n",
        "    os.makedirs(save_dir + \"/Results\")\n",
        "\n",
        "if load_checkpoint:\n",
        "    checkpoint = torch.load(save_dir + \"/Models/\" + model_name + \"_\" + str(image_size) + \".pt\", map_location = \"cpu\")\n",
        "    print(\"Checkpoint loaded\")\n",
        "    #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    vae_net.load_state_dict(checkpoint['model_state_dict'])\n",
        "    start_epoch = checkpoint[\"epoch\"]\n",
        "    loss_log = checkpoint[\"loss_log\"]\n",
        "else:\n",
        "    #If checkpoint does exist raise an error to prevent accidental overwriting\n",
        "    if os.path.isfile(save_dir + \"/Models/\" + model_name + \"_\" + str(image_size) + \".pt\"):\n",
        "        raise ValueError(\"Warning Checkpoint exists\")\n",
        "    else:\n",
        "        print(\"Starting from scratch\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ef69339",
      "metadata": {
        "id": "7ef69339",
        "outputId": "354a8b4c-aa00-409c-9fd8-ce5aeaf2c0ab"
      },
      "outputs": [],
      "source": [
        "vae_net.eval()\n",
        "\n",
        "black_image = torch.zeros((3,im_size,im_size))\n",
        "black_image = TF.to_pil_image(black_image)\n",
        "black_image = test_transform(black_image)\n",
        "black_image = black_image.unsqueeze(0).cuda()\n",
        "black_z,_,_ = vae_net.encoder(black_image)\n",
        "bl = vae_net.decoder(black_z)\n",
        "\n",
        "out = vutils.make_grid(bl, normalize=True)\n",
        "plt.imshow(out.cpu().detach().numpy().transpose((1, 2, 0)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f95d46d",
      "metadata": {
        "id": "1f95d46d",
        "outputId": "6525f0a8-feef-4d9f-9b91-e374ece9d853"
      },
      "outputs": [],
      "source": [
        "vae_net.eval()\n",
        "\n",
        "white_image = torch.ones((3,im_size,im_size))\n",
        "white_image = TF.to_pil_image(white_image)\n",
        "white_image = test_transform(white_image)\n",
        "white_image = white_image.unsqueeze(0)\n",
        "white_z,_,_ = vae_net.encoder(white_image.cuda())\n",
        "wl = vae_net.decoder(white_z)\n",
        "\n",
        "out = vutils.make_grid(wl, normalize=True)\n",
        "plt.imshow(out.cpu().detach().numpy().transpose((1, 2, 0)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45e7caee",
      "metadata": {
        "id": "45e7caee"
      },
      "outputs": [],
      "source": [
        "# Prepare Dataset for supervised training, without black/white images\n",
        "# If the model saved, you can start running the notebook here, after the imports in the first cell\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(im_size, im_size)),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.Resize(size=(im_size, im_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "class CustomImageDatasetWithLabels(Dataset):\n",
        "    def __init__(self, data_dir, file_names, labels, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.file_names = file_names\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.data_dir, self.file_names[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label_name = self.labels[idx]\n",
        "        label_index = label_to_index[label_name]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label_index\n",
        "\n",
        "\n",
        "\n",
        "def load_dataset(data_dir):\n",
        "    file_names = [f for f in os.listdir(data_dir) if f.endswith('.jpg')]  # Adjust as needed\n",
        "    labels = ['_'.join(f.split('_')[:-1]) for f in file_names]\n",
        "    return file_names, labels\n",
        "\n",
        "# Load dataset\n",
        "data_dir = dataset_root\n",
        "file_names, labels = load_dataset(data_dir)\n",
        "\n",
        "unique_labels = set()  # A set to store all unique labels\n",
        "\n",
        "# Assuming 'all_labels' is a list of all labels in the dataset\n",
        "for label in labels:\n",
        "    unique_labels.add(label)\n",
        "\n",
        "# Create a mapping from label to index\n",
        "label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Split dataset\n",
        "train_files, test_files, train_labels, test_labels = train_test_split(file_names, labels, test_size=0.3, random_state=7)\n",
        "\n",
        "train_dataset = CustomImageDatasetWithLabels(data_dir, file_names=train_files, labels=train_labels, transform=train_transform)\n",
        "test_dataset = CustomImageDatasetWithLabels(data_dir, file_names=test_files, labels=test_labels, transform=test_transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31b62924",
      "metadata": {
        "id": "31b62924"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "666433e9",
      "metadata": {
        "id": "666433e9",
        "outputId": "06d5002d-fa8d-419c-9693-48d17df0ce17"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(train_loader)\n",
        "train_images, train_labels = next(dataiter)\n",
        "train_images.shape\n",
        "\n",
        "test_dataiter = iter(test_loader)\n",
        "test_images, test_labels = next(test_dataiter)\n",
        "test_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83245474",
      "metadata": {
        "id": "83245474",
        "outputId": "fb901cea-976c-44ea-9cf1-0728f9fdcdbe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "\n",
        "class ResClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResClassifier, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.resnet.fc = nn.Identity()\n",
        "\n",
        "        # Forward a dummy variable through the feature extractor to determine input size\n",
        "        dummy_input = torch.randn(1, 3, im_size, im_size)\n",
        "        dummy_features = self.resnet(dummy_input)\n",
        "        self.feature_size = dummy_features.view(-1).shape[0]\n",
        "\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Linear(self.feature_size, 150),\n",
        "            nn.BatchNorm1d(150),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(150, 37),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        return x\n",
        "\n",
        "res_classifier = ResClassifier().to(device)\n",
        "\n",
        "print(res_classifier)\n",
        "\n",
        "\n",
        "summary(res_classifier, (3, im_size, im_size))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b8bfe1a",
      "metadata": {
        "id": "2b8bfe1a"
      },
      "outputs": [],
      "source": [
        "# Train with frozen weights for 7-10 epochs, then fine-tune the whole model for 3-5 epochs, with lr=0.0001\n",
        "num_epochs = 7\n",
        "optimizer = optim.Adam(res_classifier.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fb244ad",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            ""
          ]
        },
        "id": "5fb244ad",
        "outputId": "7f087bd5-66cf-4920-9809-95aaeed1e2c3",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "vae_net.eval()\n",
        "# Training loop\n",
        "def test_model():\n",
        "    res_classifier.eval()\n",
        "    val_loss = 0\n",
        "    val_corrects = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            reconstructed_data, _, _ = vae_net(data)\n",
        "\n",
        "            output = res_classifier(reconstructed_data)\n",
        "            val_loss = criterion(output, target)  # Sum up batch loss\n",
        "\n",
        "            _, predictions = torch.max(output.data, 1)\n",
        "\n",
        "            val_corrects += torch.sum(predictions.squeeze().int() == target.squeeze().int()).item()\n",
        "\n",
        "            val_loss += val_loss.item() * data.size(0)\n",
        "\n",
        "    # Print epoch statistics\n",
        "    epoch_loss = val_loss / len(test_loader.dataset)\n",
        "    epoch_acc = float(val_corrects) / len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        epoch_loss, val_corrects, len(test_loader.dataset),\n",
        "        100.0 * float(val_corrects) / len(test_loader.dataset)))\n",
        "\n",
        "    return epoch_loss\n",
        "\n",
        "# Existing training loop starts here...\n",
        "for epoch in range(num_epochs):\n",
        "    training_corrects = 0\n",
        "    res_classifier.train()\n",
        "    training_loss = 0\n",
        "    total_train = 0\n",
        "    for batch_idx, (data, target) in enumerate(tqdm(train_loader, leave=False)):\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        #target = target.unsqueeze(1).float()\n",
        "        reconstructed_data, _, _ = vae_net(data) # Reconstructions from the VAE are fed to the classifier\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = res_classifier(reconstructed_data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        training_loss += loss.item() * data.size(0)\n",
        "        _, predictions = torch.max(output.data, 1)\n",
        "\n",
        "        training_corrects += torch.sum(predictions.squeeze().int() == target.squeeze().int()).item()\n",
        "\n",
        "\n",
        "    epoch_loss = training_loss / len(train_loader.dataset)\n",
        "    epoch_acc = float(training_corrects) / len(train_loader.dataset)\n",
        "\n",
        "    print('Epoch: {} Average training loss: {:.4f}, Training Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        epoch, epoch_loss, training_corrects, len(train_loader.dataset),\n",
        "        100.0 * float(training_corrects) / len(train_loader.dataset)))\n",
        "    # Test the model at the end of each epoch\n",
        "    test_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "353c540d",
      "metadata": {
        "id": "353c540d"
      },
      "outputs": [],
      "source": [
        "for param in res_classifier.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbaf2ef0",
      "metadata": {
        "id": "cbaf2ef0"
      },
      "outputs": [],
      "source": [
        "\n",
        "torch.save({\n",
        "                    'epoch'                         : epoch,\n",
        "                    'loss_log'                      : loss_log,\n",
        "                    'model_state_dict'              : res_classifier.state_dict(),\n",
        "                    'optimizer_state_dict'          : optimizer.state_dict()\n",
        "\n",
        "                     }, save_dir + \"/Models/\" + model_name + \"_classifier_\" + str(image_size) + \".pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52a5fa28",
      "metadata": {
        "id": "52a5fa28",
        "outputId": "109f7137-2624-492b-a829-23eae48cff39"
      },
      "outputs": [],
      "source": [
        "if not os.path.isdir(save_dir + \"/Models\"):\n",
        "    os.makedirs(save_dir + \"/Models\")\n",
        "if not os.path.isdir(save_dir + \"/Results\"):\n",
        "    os.makedirs(save_dir + \"/Results\")\n",
        "\n",
        "if load_checkpoint:\n",
        "    checkpoint = torch.load(save_dir + \"/Models/\" + model_name + \"_classifier_\" + str(image_size) + \".pt\", map_location = \"cpu\")\n",
        "    print(\"Checkpoint loaded\")\n",
        "    res_classifier.load_state_dict(checkpoint['model_state_dict'])\n",
        "else:\n",
        "    #If checkpoint does exist raise an error to prevent accidental overwriting\n",
        "    if os.path.isfile(save_dir + \"/Models/\" + model_name + \"_\" + str(image_size) + \".pt\"):\n",
        "        raise ValueError(\"Warning Checkpoint exists\")\n",
        "    else:\n",
        "        print(\"Starting from scratch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52df8d63",
      "metadata": {
        "id": "52df8d63"
      },
      "outputs": [],
      "source": [
        "# Geodesic Path Algorithm\n",
        "\n",
        "import torch\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.manual_seed(0)\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def compute_etta(model, zi, zi_minus, zi_plus, dt):\n",
        "    #model.eval()\n",
        "    g_zi_minus = model.decoder(zi_minus).view(-1)\n",
        "    g_zi = model.decoder(zi).view(-1)\n",
        "    g_zi_plus = model.decoder(zi_plus).view(-1)\n",
        "\n",
        "    finite_diff = (g_zi_plus - 2*g_zi + g_zi_minus) / dt\n",
        "\n",
        "\n",
        "\n",
        "    # Use the encoder's Jacobian to map the finite difference in X space back to Z space\n",
        "    Jh_tuple = torch.autograd.functional.jacobian(model.encoder, g_zi.view(1,3,image_size,image_size))\n",
        "    Jh = Jh_tuple[0].view(512*4*4, -1)  # Reshape to a 2D tensor\n",
        "\n",
        "    etta_i = - torch.mm(Jh, finite_diff.unsqueeze(-1)).view_as(zi)\n",
        "    del Jh\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return etta_i\n",
        "\n",
        "def compute_etta_d(model, zi, zi_minus, zi_plus, dt):\n",
        "    # Compute the finite difference\n",
        "    g_zi_minus = model.decoder(zi_minus).view(-1)\n",
        "    g_zi = model.decoder(zi).view(-1)\n",
        "    g_zi_plus = model.decoder(zi_plus).view(-1)\n",
        "\n",
        "    finite_diff = (g_zi_plus - 2 * g_zi + g_zi_minus) / dt\n",
        "    finite_diff = finite_diff.view(1, 3, image_size, image_size)  # Reshape it to match the encoder's output shape\n",
        "\n",
        "    scaled_finite_diff = 0.1 * finite_diff\n",
        "\n",
        "    # Compute Jacobian-vector product\n",
        "    vjp_outputs = torch.autograd.functional.vjp(model.decoder, zi, finite_diff)\n",
        "    #print(\"vjp\", len(vjp_outputs))\n",
        "    # Get the result from the vjp outputs\n",
        "    Jv = vjp_outputs[1].view_as(zi)\n",
        "    #print(\"Jv\", Jv.shape)\n",
        "\n",
        "    # Compute etta_i\n",
        "    etta_i = -Jv #+ scaled_finite_diff)\n",
        "\n",
        "    # Free up memory\n",
        "    del g_zi_minus, g_zi, g_zi_plus, finite_diff, Jv, vjp_outputs\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return etta_i\n",
        "\n",
        "def compute_etta1(model, zi, zi_minus, zi_plus, dt):\n",
        "\n",
        "    g_zi_minus = model.decoder(zi_minus).view(-1).detach()\n",
        "    g_zi = model.decoder(zi).view(-1)  # We will need the gradient information for this tensor for the Jacobian\n",
        "    g_zi_plus = model.decoder(zi_plus).view(-1).detach()\n",
        "\n",
        "    finite_diff = (g_zi_plus - 2 * g_zi + g_zi_minus) / dt\n",
        "\n",
        "    def partial_encoder(input_data):\n",
        "        return model.encoder(input_data)[1]\n",
        "\n",
        "    #encoder_output = model.encoder(model.decoder(zi))  # Assuming input_data is your input to the encoder\n",
        "      # Access the first element of the tuple\n",
        "    Jh_tuple = torch.autograd.functional.jacobian(partial_encoder, g_zi.view(1, 3, im_size, im_size))\n",
        "    Jh = Jh_tuple.view(512 * 4 * 4, -1)  # Reshape to a 2D tensor\n",
        "\n",
        "\n",
        "    # Use the encoder's Jacobian to map the finite difference in X space back to Z space\n",
        "    #Jh_tuple = torch.autograd.functional.jacobian(model.encoder, g_zi.view(1, 3, im_size, im_size))\n",
        "    #Jh = Jh_tuple[0].view(512 * 4 * 4, -1)  # Reshape to a 2D tensor\n",
        "\n",
        "    etta_i = - torch.mm(Jh, finite_diff.unsqueeze(-1)).view_as(zi)\n",
        "\n",
        "    # Free up memory\n",
        "    del g_zi_minus, g_zi, g_zi_plus, Jh, Jh_tuple, finite_diff\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return etta_i\n",
        "\n",
        "\n",
        "def compute_etta(model, zi, zi_minus, zi_plus, dt):\n",
        "    # Compute the finite difference\n",
        "    g_zi_minus = model.decoder(zi_minus).view(-1)\n",
        "    g_zi = model.decoder(zi).view(-1)\n",
        "    g_zi_plus = model.decoder(zi_plus).view(-1)\n",
        "\n",
        "    finite_diff = (g_zi_plus - 2 * g_zi + g_zi_minus) / dt\n",
        "    finite_diff = finite_diff.view(1, 3, image_size, image_size)  # Reshape it to match the encoder's output shape\n",
        "\n",
        "    # Define a wrapper function for the encoder, so it can handle just the required output\n",
        "    def partial_encoder(input_data):\n",
        "        return model.encoder(input_data)[1]\n",
        "\n",
        "    # Compute Jacobian-vector product\n",
        "    #vjp_outputs = torch.autograd.functional.jvp(wrapper_decoder, zi, finite_diff)\n",
        "    vjp_outputs = torch.autograd.functional.jvp(partial_encoder, g_zi.view(1, 3, image_size, image_size), finite_diff)\n",
        "    # vjp_outputs = torch.autograd.functional.jvp(wrapper_func, g_zi.view(1, 3, 64, 64), finite_diff)\n",
        "\n",
        "    # Get the result from the vjp outputs\n",
        "    Jv = vjp_outputs[1].view_as(zi)\n",
        "\n",
        "    # Compute etta_i\n",
        "    etta_i = -Jv\n",
        "\n",
        "    # Free up memory\n",
        "    del g_zi_minus, g_zi, g_zi_plus, finite_diff, Jv, vjp_outputs\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return etta_i\n",
        "\n",
        "\n",
        "def compute_etta_d(model, zi, zi_minus, zi_plus, dt):\n",
        "    # Compute the finite difference\n",
        "    g_zi_minus = model.decoder(zi_minus).view(-1)\n",
        "    g_zi = model.decoder(zi).view(-1)\n",
        "    g_zi_plus = model.decoder(zi_plus).view(-1)\n",
        "\n",
        "    finite_diff = (g_zi_plus - 2 * g_zi + g_zi_minus) / dt\n",
        "    finite_diff = finite_diff.view(1, 3, image_size, image_size)  # Reshape it to match the encoder's output shape\n",
        "\n",
        "    # Compute Jacobian-vector product\n",
        "    vjp_outputs = torch.autograd.functional.vjp(model.decoder, zi, finite_diff)\n",
        "    #print(\"vjp\", len(vjp_outputs))\n",
        "    # Get the result from the vjp outputs\n",
        "    Jv = vjp_outputs[1].view_as(zi)\n",
        "\n",
        "    # Compute etta_i\n",
        "    etta_i = -Jv\n",
        "\n",
        "    # Free up memory\n",
        "    del g_zi_minus, g_zi, g_zi_plus, finite_diff, Jv, vjp_outputs\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return etta_i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2b246ea",
      "metadata": {
        "id": "f2b246ea",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def backtracking_line_search(model, z_collection, i, direction, start_alpha, beta, dt, max_iterations, c=0.001):\n",
        "    alpha = start_alpha\n",
        "    current_energy = sum_of_etta_norms(model, z_collection, dt)\n",
        "    gradient_norm_square = direction.norm().pow(2)\n",
        "\n",
        "\n",
        "    tmp_z = z_collection[i] - alpha * direction\n",
        "    new_z_collection = [element.clone() for element in z_collection]\n",
        "    new_z_collection[i] = tmp_z\n",
        "    #iterations_count = 0\n",
        "\n",
        "    while sum_of_etta_norms(model, new_z_collection, dt) > current_energy - c * alpha * gradient_norm_square:\n",
        "        alpha *= beta\n",
        "        tmp_z = z_collection[i] - alpha * direction\n",
        "        new_z_collection[i] = tmp_z\n",
        "        #iterations_count+=1\n",
        "\n",
        "    return alpha\n",
        "\n",
        "def sum_of_etta_norms(model, z_collection, dt):\n",
        "    norms = []\n",
        "    for j in range(1, len(z_collection) - 1):\n",
        "        etta_j = compute_etta_d(model, z_collection[j], z_collection[j-1], z_collection[j+1], dt)\n",
        "        norms.append(etta_j.norm().pow(2).item())\n",
        "        #del etta_j\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    return sum(norms)\n",
        "\n",
        "\n",
        "def geodesic_path_algorithm(model, z_collection, alpha, T, beta, epsilon, max_iterations):\n",
        "    model.eval()\n",
        "    dt = 1.0 / T\n",
        "    initial_sum_norms = float('inf')\n",
        "    iterations = 0\n",
        "\n",
        "    while sum_of_etta_norms(model, z_collection, dt) > epsilon:\n",
        "        print(f\"It{iterations}:sum_etta_norms\", sum_of_etta_norms(model, z_collection, dt))\n",
        "\n",
        "    #while True:\n",
        "        etta_norms = []\n",
        "        if sum_of_etta_norms(model, z_collection, dt) > initial_sum_norms:\n",
        "                initial_sum_norms = sum_of_etta_norms(model, z_collection, dt)\n",
        "        else:\n",
        "            pass\n",
        "            #break\n",
        "        #while True:\n",
        "            #if init == 0:\n",
        "        if iterations == max_iterations:\n",
        "            break\n",
        "\n",
        "        iterations +=1\n",
        "\n",
        "        for i in range(1, T-1):\n",
        "            etta_i = compute_etta_d(model, z_collection[i], z_collection[i-1], z_collection[i+1], dt)\n",
        "            # backtracking_line_search is time consuming, instead use small value for alpha and run for sufficient iterations ~ 200 iterations\n",
        "            #alpha_i = backtracking_line_search(model, z_collection, i, etta_i, alpha, beta, dt, max_iterations)\n",
        "            alpha_i= alpha\n",
        "            z_collection[i] -= alpha_i * etta_i\n",
        "            etta_norms.append(etta_i.norm().pow(2).item())\n",
        "\n",
        "            del etta_i\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "        geodesic_path = z_collection\n",
        "    return z_collection\n",
        "\n",
        "# Example usage:\n",
        "vae_net.eval()\n",
        "image_size = im_size\n",
        "x,_,_ =vae_net.encoder(train_images[1].view(1,3,128,128).cuda())\n",
        "zero_z = torch.FloatTensor(1,64,8,8).zero_().cuda()\n",
        "#black_image = torch.FloatTensor(1,3,128,128).zero_().cuda()\n",
        "#black_z, _, _= vae_net.encoder(black_image)\n",
        "#white_image = torch.ones(1,3,128,128).cuda()\n",
        "#white_z, _, _= vae_net.encoder(white_image)\n",
        "#z_0,_,_ = vae_net.encoder(train_images[3].view(1,3,128,128).cuda())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fa418a5",
      "metadata": {
        "id": "9fa418a5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def integrated_gradients(model, baseline, image, target_class, steps=20):\n",
        "    \"\"\"\n",
        "    Compute the integrated gradients for a given model and image.\n",
        "\n",
        "    Parameters:\n",
        "    - model: the PyTorch model (should be in eval mode)\n",
        "    - baseline: the baseline image (usually a tensor of zeros with the same shape as the image)\n",
        "    - image: the input image tensor\n",
        "    - target_class: the index of the target class for which gradients should be calculated\n",
        "    - steps: the number of steps in the Riemann sum approximation of the integral\n",
        "\n",
        "    Returns:\n",
        "    - Integrated gradients with respect to the input image\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure model is in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Generate scaled versions of the image\n",
        "    scaled_images = [baseline + (float(i) / steps) * (image - baseline) for i in range(0, steps)]\n",
        "\n",
        "    # Convert to tensor\n",
        "    scaled_images = torch.cat([img.unsqueeze(0) for img in scaled_images], dim=0)\n",
        "\n",
        "    scaled_images = scaled_images.view(steps, 3,image_size,image_size)\n",
        "\n",
        "    # Require gradient\n",
        "    #scaled_images.requires_grad_(True)\n",
        "    scaled_images = scaled_images.requires_grad_(True)\n",
        "\n",
        "    # Get model predictions for the scaled images\n",
        "    model_outputs = model(scaled_images)\n",
        "    m = torch.nn.Softmax(dim=1)\n",
        "    model_outputs = m(model_outputs)\n",
        "\n",
        "    # Extract the scores of the target class\n",
        "    scores = model_outputs[:, target_class]\n",
        "    #print(scores[-1])\n",
        "\n",
        "    # Compute gradients\n",
        "    #scores.backward(torch.ones_like(scores))\n",
        "\n",
        "    gradients = torch.autograd.grad(outputs=scores, inputs=scaled_images,\n",
        "                                    grad_outputs=torch.ones(scores.size()).to(image.device),\n",
        "                                    create_graph=True)[0]\n",
        "\n",
        "\n",
        "    # Average the gradients across all steps\n",
        "    avg_gradients = torch.mean(gradients, dim=0)\n",
        "\n",
        "    # Compute the integrated gradients\n",
        "    integrated_gradients = (image - baseline) * avg_gradients\n",
        "\n",
        "    return integrated_gradients\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a96be056",
      "metadata": {
        "id": "a96be056"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def integrated_gradients_manifold(model, manifold_imgs, target_class, steps=20):\n",
        "    \"\"\"\n",
        "    Compute the integrated gradients for a given model and image.\n",
        "\n",
        "    Parameters:\n",
        "    - model: the PyTorch model (should be in eval mode)\n",
        "    - baseline: the baseline image (usually a tensor of zeros with the same shape as the image)\n",
        "    - image: the input image tensor\n",
        "    - target_class: the index of the target class for which gradients should be calculated\n",
        "    - steps: the number of steps in the Riemann sum approximation of the integral\n",
        "\n",
        "    Returns:\n",
        "    - Integrated gradients with respect to the input image\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure model is in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Generate scaled versions of the image\n",
        "    scaled_images = manifold_imgs #[baseline + (float(i) / steps) * (image - baseline) for i in range(0, steps)]\n",
        "\n",
        "    # Convert to tensor\n",
        "    scaled_images = torch.cat([img.unsqueeze(0) for img in scaled_images], dim=0)\n",
        "\n",
        "    scaled_images = scaled_images.view(steps, 3,image_size,image_size)\n",
        "\n",
        "    # Require gradient\n",
        "    #scaled_images.requires_grad_(True)\n",
        "    scaled_images = scaled_images.requires_grad_(True)\n",
        "\n",
        "    # Get model predictions for the scaled images\n",
        "    model_outputs = model(scaled_images)\n",
        "    m = torch.nn.Softmax(dim=1)\n",
        "    model_outputs = m(model_outputs)\n",
        "\n",
        "    # Extract the scores of the target class\n",
        "    scores = model_outputs[:, target_class]\n",
        "    print(scores[-1])\n",
        "\n",
        "    # Retrieve gradients; gradients are now a tensor of the same shape as scaled_images\n",
        "    gradients = torch.autograd.grad(outputs=scores, inputs=scaled_images,\n",
        "                                    grad_outputs=torch.ones(scores.size()).to(scaled_images[0].device),\n",
        "                                    create_graph=True)[0]\n",
        "\n",
        "\n",
        "    # Average the gradients across all steps\n",
        "    avg_gradients = torch.mean(gradients, dim=0)\n",
        "    baseline = torch.zeros_like(im).cuda()\n",
        "\n",
        "    # Compute the integrated gradients\n",
        "    integrated_gradients = (scaled_images[-1] - scaled_images[0]) * avg_gradients\n",
        "\n",
        "    return integrated_gradients, avg_gradients\n",
        "\n",
        "# Calculate integrated gradients\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dcb2cc4",
      "metadata": {
        "id": "8dcb2cc4",
        "outputId": "478abdd1-fac4-486f-ac1e-89617858c257"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20,10))\n",
        "out = vutils.make_grid(test_images[40:60], normalize=True)\n",
        "plt.imshow(out.numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "365e7541",
      "metadata": {
        "id": "365e7541",
        "outputId": "c7f67e5e-848b-467b-8f9c-a2884b28aca9"
      },
      "outputs": [],
      "source": [
        "#8, 29\n",
        "\n",
        "#43, 49\n",
        "\n",
        "res_classifier.eval()\n",
        "ix = 43\n",
        "im, label = test_images[ix].unsqueeze(0).cuda(), test_labels[ix]\n",
        "rec_im,_,_ = vae_net(im)\n",
        "#x = im\n",
        "output = res_classifier(rec_im)\n",
        "_, predictions = torch.max(output.data, 1)\n",
        "target_class = predictions\n",
        "target_class == label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed44ba93",
      "metadata": {
        "id": "ed44ba93",
        "outputId": "f7d83110-512b-44b8-ae76-1d4d204afbf6"
      },
      "outputs": [],
      "source": [
        "#linear path in the latent space\n",
        "def interpolate(start, end, steps):\n",
        "    \"\"\"Generate interpolated vectors between start and end.\"\"\"\n",
        "    interpolation = [start + (float(i)/steps)*(end-start) for i in range(0, steps)]\n",
        "    return interpolation\n",
        "\n",
        "im = train_images[1].unsqueeze(0).cuda()\n",
        "vae_net.eval\n",
        "x_z,_,_ = vae_net.encoder(im)\n",
        "rand_z = torch.rand_like(x_z)\n",
        "zeros_z = torch.zeros_like(x_z)\n",
        "ones_z = torch.ones_like(x_z)\n",
        "\n",
        "interpolated_vectors = interpolate(black_z, x_z, 20)\n",
        "\n",
        "interpolated_rec_images = [vae_net.decoder(vec) for vec in interpolated_vectors]\n",
        "\n",
        "vv = [v.squeeze() for v in interpolated_rec_images]\n",
        "plt.figure(figsize = (20,10))\n",
        "out = vutils.make_grid(vv, normalize=True)\n",
        "plt.imshow(out.cpu().numpy().transpose((1, 2, 0)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vTKgdv5HMWUR",
      "metadata": {
        "id": "vTKgdv5HMWUR"
      },
      "outputs": [],
      "source": [
        "# geodesic path in the latent space\n",
        "T = 20\n",
        "z0 = black_z\n",
        "zT = x_z\n",
        "z_collection = [z0 + float(i) / T * (zT - z0) for i in range(T)] # initial linear path\n",
        "#z_collection = [z0 if i == 0 else zT if i == T - 1 else z0 + float(i) / T * (zT - z0) + 0.8 * torch.randn_like(z0) for i in range(T)] # linear path with added noise\n",
        "#M = 0.5 * (z0 + zT) $ For cubic path\n",
        "\n",
        "#z_collection = [((1 - float(i)/T)**2) * z0 + 2*(1 - float(i)/T)*float(i)/T * M + (float(i)/T)**2 * zT for i in range(T)]\n",
        "path = geodesic_path_algorithm(vae_net, z_collection, alpha=0.00005, T=20, beta=0.5, epsilon=1000, max_iterations=200)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sGJ81h9mLCuF",
      "metadata": {
        "id": "sGJ81h9mLCuF"
      },
      "outputs": [],
      "source": [
        "interpolated_geodesic_images = [vae_net.decoder(vec) for vec in path]\n",
        "\n",
        "vv = [v.squeeze() for v in interpolated_geodesic_images]\n",
        "plt.figure(figsize = (20,10))\n",
        "out = vutils.make_grid(vv, normalize=True)\n",
        "plt.imshow(out.cpu().numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55649de4",
      "metadata": {
        "id": "55649de4",
        "outputId": "acb01397-b371-4b2a-ac82-59f8d8690e22"
      },
      "outputs": [],
      "source": [
        "#linear path in the original image space\n",
        "interpolated_vectors = interpolate(black_image, im, 20)\n",
        "\n",
        "\n",
        "vv = [v.squeeze() for v in interpolated_vectors]\n",
        "plt.figure(figsize = (20,10))\n",
        "out = vutils.make_grid(vv, normalize=True)\n",
        "plt.imshow(out.cpu().numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a75f704",
      "metadata": {
        "id": "0a75f704",
        "outputId": "75e4023b-360d-4911-cf0d-b3edd447c777"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "\n",
        "from captum.attr import IntegratedGradients\n",
        "from captum.attr import GradientShap\n",
        "from captum.attr import Occlusion\n",
        "from captum.attr import NoiseTunnel\n",
        "from captum.attr import visualization as viz\n",
        "os.environ['TORCH_HOME'] = 'models\\\\'\n",
        "os.getcwd()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f663c48",
      "metadata": {
        "id": "4f663c48",
        "outputId": "57199dc4-0bdb-4038-8115-daff3234932d"
      },
      "outputs": [],
      "source": [
        "zero_baseline = torch.zeros_like(rec_im).cuda() # Assuming image is your input tensor\n",
        "to_pil_image = transforms.ToPILImage()\n",
        "baseline_pil = to_pil_image(zero_baseline[0].cpu())\n",
        "\n",
        "baseline_transformed = test_transform(baseline_pil)\n",
        "baseline_transformed = baseline_transformed.unsqueeze(0).cuda()\n",
        "# Calculate integrated gradients\n",
        "\n",
        "int_grads_ = integrated_gradients(res_classifier, zero_baseline, rec_im, target_class, steps=20)\n",
        "default_cmap = LinearSegmentedColormap.from_list('custom blue',\n",
        "                                                 [(0, '#000000'),\n",
        "                                                  (0.25, '#ffffff'),\n",
        "                                                  (1, '#ffffff')], N=256)\n",
        "_ = viz.visualize_image_attr(np.transpose(int_grads_.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                             np.transpose(im.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                             method='heat_map',\n",
        "                             cmap=default_cmap,\n",
        "                             show_colorbar=True,\n",
        "                             sign='absolute_value',\n",
        "                             outlier_perc=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9485e218",
      "metadata": {
        "id": "9485e218"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c88c9ddf",
      "metadata": {
        "id": "c88c9ddf",
        "outputId": "7cbd6e7e-6c28-474d-9f1e-83482d1bde5a"
      },
      "outputs": [],
      "source": [
        "#IG on linear latent\n",
        "int_grads_lin, av = integrated_gradients_manifold(res_classifier, interpolated_rec_images, target_class, steps=20)\n",
        "default_cmap = LinearSegmentedColormap.from_list('custom blue',\n",
        "                                                 [(0, '#000000'),\n",
        "                                                  (0.25, '#ffffff'),\n",
        "                                                  (1, '#ffffff')], N=256)\n",
        "\n",
        "_ = viz.visualize_image_attr(np.transpose(int_grads_lin.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                             np.transpose(im.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                             method='heat_map',\n",
        "                             cmap=default_cmap,\n",
        "                             show_colorbar=True,\n",
        "                             sign='absolute_value',\n",
        "                             outlier_perc=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oNtBlSt1Ngcb",
      "metadata": {
        "id": "oNtBlSt1Ngcb"
      },
      "outputs": [],
      "source": [
        "#IG on geodesic latent\n",
        "\n",
        "int_grads_geo, av = integrated_gradients_manifold(res_classifier, interpolated_geodesic_images, target_class, steps=20)\n",
        "default_cmap = LinearSegmentedColormap.from_list('custom blue',\n",
        "                                                 [(0, '#000000'),\n",
        "                                                  (0.25, '#ffffff'),\n",
        "                                                  (1, '#ffffff')], N=256)\n",
        "\n",
        "_ = viz.visualize_image_attr(np.transpose(int_grads_geo.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                             np.transpose(im.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                             method='heat_map',\n",
        "                             cmap=default_cmap,\n",
        "                             show_colorbar=True,\n",
        "                             sign='absolute_value',\n",
        "                             outlier_perc=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d53fea12",
      "metadata": {
        "id": "d53fea12",
        "outputId": "456859ab-6d5a-41ba-ca00-a3d6e60f3946"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20,10))\n",
        "out = vutils.make_grid(train_images[0:20], normalize=True)\n",
        "plt.imshow(out.numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd976b19",
      "metadata": {
        "id": "bd976b19"
      },
      "outputs": [],
      "source": [
        "\n",
        "def normalize(exp):\n",
        "    exp = torch.sum(torch.abs(exp), dim=1)\n",
        "\n",
        "    exp = exp / torch.sum(exp)\n",
        "    return exp\n",
        "\n",
        "def clamp(x, mean, std):\n",
        "    \"\"\"\n",
        "    Helper method for clamping the adversarial example in order to ensure that it is a valid image\n",
        "    \"\"\"\n",
        "    upper = torch.from_numpy(np.array((1.0 - mean) / std)).to(x.device)\n",
        "    lower = torch.from_numpy(np.array((0.0 - mean) / std)).to(x.device)\n",
        "\n",
        "    if x.shape[1] == 3:  # 3-channel image\n",
        "        for i in [0, 1, 2]:\n",
        "            x[0][i] = torch.clamp(x[0][i], min=lower[i], max=upper[i])\n",
        "    else:\n",
        "        x = torch.clamp(x, min=lower[0], max=upper[0])\n",
        "    return x\n",
        "\n",
        "data_mean = np.array([0.485, 0.456, 0.406])\n",
        "data_std = np.array([0.229, 0.224, 0.225])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5a72ef9",
      "metadata": {
        "id": "d5a72ef9",
        "outputId": "f8583803-2a47-4055-fe2d-8964c497ef17",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Targeted Attributional attack, see \"Explanations can be manipulated and geometry is to blame\"\n",
        "\n",
        "\n",
        "\n",
        "def get_expl(model, x_rec, baseline):\n",
        "\n",
        "    output = model(x_rec)\n",
        "    _, predictions = torch.max(output.data, 1)\n",
        "    target_class = predictions\n",
        "    m = torch.nn.Softmax(dim=1)\n",
        "    model_outputs = m(output)\n",
        "    scores = model_outputs[:, target_class]\n",
        "    exp = integrated_gradients(model, baseline, x_rec, target_class, steps=20)\n",
        "    return exp, scores, target_class\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x,_,_ = vae_net(test_images[43].unsqueeze(0).cuda())\n",
        "x_target,_,_ = vae_net(test_images[49].unsqueeze(0).cuda())\n",
        "x_adv = x.clone().detach().requires_grad_()\n",
        "\n",
        "\n",
        "\n",
        "baseline = zero_baseline\n",
        "# produce expls\n",
        "org_expl, org_acc, org_idx = get_expl(res_classifier, x, baseline)\n",
        "#org_expl = org_expl.detach().cpu()\n",
        "target_expl, _, _ = get_expl(res_classifier, x_target, baseline)\n",
        "target_expl = target_expl.detach()\n",
        "#target_expl =  normalize(target_expl)\n",
        "optimizer_adv = torch.optim.Adam([x_adv], lr=0.07)\n",
        "\n",
        "for i in range(1000):\n",
        "        optimizer_adv.zero_grad()\n",
        "\n",
        "        # calculate loss\n",
        "        adv_expl, adv_acc, class_idx = get_expl(res_classifier, x_adv, baseline)\n",
        "        #adv_expl = normalize(adv_expl)\n",
        "        loss_expl = F.mse_loss(adv_expl, target_expl)\n",
        "        loss_output = F.mse_loss(adv_acc, org_acc.detach())\n",
        "        input_loss = F.mse_loss(x.detach(), x_adv.detach())\n",
        "        total_loss = 1.0e17*loss_expl + 5.0e6 *loss_output + 1e3 *input_loss\n",
        "\n",
        "        # update adversarial example\n",
        "        total_loss.backward()\n",
        "        optimizer_adv.step()\n",
        "        if class_idx != org_idx:\n",
        "            print(\"class index changed\")\n",
        "            break\n",
        "\n",
        "        # clamp adversarial example\n",
        "        # Note: x_adv.data returns tensor which shares data with x_adv but requires\n",
        "        #       no gradient. Since we do not want to differentiate the clamping,\n",
        "        #       this is what we need\n",
        "        x_adv.data = clamp(x_adv.data, data_mean, data_std)\n",
        "\n",
        "        print(\"It-{}: Tot_Lss: {}, Expl_Lss: {}, Out_Lss: {} , inp_Lss: {}\".format(i, total_loss.item(),\n",
        "                                                                                   loss_expl.item(),\n",
        "                                                                                   loss_output.item(),input_loss.item()))\n",
        "\n",
        "    # test with original model (with relu activations)\n",
        "adv_expl, adv_acc, class_idx = get_expl(res_classifier, x_adv, baseline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a6ca17b",
      "metadata": {
        "id": "3a6ca17b"
      },
      "outputs": [],
      "source": [
        "#Check if adversarial image still has the same output\n",
        "class_idx == org_idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ba76169",
      "metadata": {
        "id": "5ba76169",
        "outputId": "17bb08b8-d972-4d63-ddb5-908f0ffd38aa"
      },
      "outputs": [],
      "source": [
        "default_cmap = LinearSegmentedColormap.from_list('custom blue',\n",
        "                                                 [(0, '#000000'),\n",
        "                                                  (0.25, '#ffffff'),\n",
        "                                                  (1, '#ffffff')], N=256)\n",
        "\n",
        "_ = viz.visualize_image_attr(np.transpose(adv_expl.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                             np.transpose(im.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                             method='heat_map',\n",
        "                             cmap=default_cmap,\n",
        "                             show_colorbar=True,\n",
        "                             sign='absolute_value',\n",
        "                             outlier_perc=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6206635d",
      "metadata": {
        "id": "6206635d",
        "outputId": "38376878-1917-42cc-9faa-6f08569187da"
      },
      "outputs": [],
      "source": [
        "\n",
        "default_cmap = LinearSegmentedColormap.from_list('custom blue',\n",
        "                                                 [(0, '#000000'),\n",
        "                                                  (0.25, '#ffffff'),\n",
        "                                                  (1, '#ffffff')], N=256)\n",
        "\n",
        "_ = viz.visualize_image_attr(np.transpose(org_expl.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                             np.transpose(im.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                             method='heat_map',\n",
        "                             cmap=default_cmap,\n",
        "                             show_colorbar=True,\n",
        "                             sign='absolute_value',\n",
        "                             outlier_perc=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15710fb3",
      "metadata": {
        "id": "15710fb3",
        "outputId": "f246dc46-cb1b-4176-b8dd-bd128a0eea96"
      },
      "outputs": [],
      "source": [
        "target_expl, _, _ = get_expl(res_classifier, x_target, baseline)\n",
        "default_cmap = LinearSegmentedColormap.from_list('custom blue',\n",
        "                                                 [(0, '#000000'),\n",
        "                                                  (0.25, '#ffffff'),\n",
        "                                                  (1, '#ffffff')], N=256)\n",
        "\n",
        "_ = viz.visualize_image_attr(np.transpose(target_expl.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                             np.transpose(im.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                             method='heat_map',\n",
        "                             cmap=default_cmap,\n",
        "                             show_colorbar=True,\n",
        "                             sign='absolute_value',\n",
        "                             outlier_perc=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7118ade4",
      "metadata": {
        "id": "7118ade4",
        "outputId": "781a2e5d-d3da-4386-ff9d-3757748aefb3",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20,10))\n",
        "out = vutils.make_grid(x_adv.squeeze().cpu().detach(), normalize=True)\n",
        "plt.imshow(out.numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dde4a957",
      "metadata": {
        "id": "dde4a957",
        "outputId": "6e9ba073-c97c-47c9-bae1-9a6f73c7e829"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20,10))\n",
        "out = vutils.make_grid(x.squeeze().cpu().detach(), normalize=True)\n",
        "plt.imshow(out.numpy().transpose((1, 2, 0)))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
